% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PROBLEM SET LATEX TEMPLATE FILE
% DEFINE DOCUMENT STYLE, LOAD PACKAGES
\documentclass[11pt,notitlepage]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}    % ADD COMMENTS USING A PERCENT SIGN
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath, booktabs}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multicol}
\usepackage{multirow}
\setlength{\parindent}{0in}  	% uncomment to remove indent at start of paragraphs
\usepackage{pdflscape}
\usepackage[english]{babel}
\usepackage[pdftex]{hyperref}
\usepackage{natbib}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphics}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{latexsym}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{layouts} 
\usepackage[titletoc]{appendix}
\DeclareGraphicsExtensions{.pdf,.jpg,.png}
\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage{float}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
% FONTS
\usepackage[T1]{fontenc}					% always use this no matter what

\usepackage{xcolor}
\usepackage[printwatermark]{xwatermark}
\newwatermark[allpages,color=black!50,angle=45,scale=1,xpos=0,ypos=0]{DO NOT DISTRIBUTE}


% uncomment any one of these to see what it does to your font!
%\usepackage{pxfonts}
%\usepackage{cmbright}
%\usepackage{txfonts}
%\usepackage[adobe-utopia]{mathdesign}
%\usepackage{kpfonts}
%\usepackage{lmodern}
%\usepackage{newtxtext,newtxmath}




% DEFINE WHAT GOES INTO YOUR TITLE BEFORE THE DOCUMENT BEGINS
\title{Field Experiments: Design, Analysis and Interpretation \\
Solutions for Chapter 4 Exercises}
\author{Alan S. Gerber and Donald P. Green\footnote{Solutions prepared by Peter M. Aronow and revised by Alexander Coppock}}
\date{\today}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle


\section*{Question 1}
Important concepts:

\begin{enumerate}[a)]
\item Define ``covariate.''  Explain why covariates are (at least in principle) measured prior to the random allocation of subjects to treatment and control.\\
Answer:\\
A covariate is a variable that is (1) unaffected by the treatment and (2) used to predict outcomes. In order to increase the credibility of the claim that a given covariate is unaffected by the treatment, researchers typically restrict the set of covariates to those variables that are measured (or are measurable) prior to the random allocation of treatments.

\item Define ``disturbance term.''  \\
Answer:\\
The disturbance term comprises all sources of variation in potential outcomes other than the average treatment effect.  For example, in equation (4.7), the disturbance term is $u_i=Y_i (0)-\mu_{Y(0)}+[(Y_i (1)-\mu_{Y(1)} )-(Y_i (0)-\mu_{Y(0)})] D_i$.  The disturbance term comprises the idiosyncratic variation in untreated responses $Y_i (0)-\mu_{Y(0)}$, plus the idiosyncratic variation in treatment effects $[(Y_i (1)-\mu_{Y(1)})-(Y_i (0)-\mu_{Y(0)})] D_i.$  

\item In equation (4.2), we demonstrated that rescaling the outcome by subtracting a pre-test leads to unbiased estimates of the ATE. Suppose that instead of subtracting the pre-test $X_i$, we subtracted a rescaled pretest $cX_i$, where $c$ is some positive constant.  Show that this procedure produces unbiased estimates of the ATE. \\
Answer:\\
The proof is similar to equation (4.2) and again makes use of the fact that the expected value of $X_i$ is the same in the treatment and control groups when treatments are allocated randomly:

\begin{align*}
E[\widehat{ATE}] &=E[Y_i-cX_i |D_i=1]-E[Y_i-cX_i | D_i=0)]\\
&= E[Y_i|D_i=1]-E[cX_i|D_i=1]-E[Y_i|D_i=0]+E[cX_i|D_i=0]\\
&= E[Y_i|D_i=1]-cE[X_i|D_i=1]-E[Y_i|D_i=0]+cE[X_i|D_i=0]\\
&= E[Y_i(1)] - E[Y_i(0)]
\end{align*}

\item Show that the parameter $b$ in equation (4.7) is identical to the ATE.\\
Answer:\\
Recall from Equation (4.7) that:
\begin{align*}
Y_i & =Y_i (0)(1-D_i )+Y_i (1) D_i\\
&=Y_i (0)+(Y_i (1)-Y_i (0) )D_i\\
&= \mu_{Y(0)}+[\mu_{Y(1)}-\mu_{Y(0)} ] D_i+Y_i (0)-\mu_{Y(0)}+[(Y_i (1)-\mu_{Y(1)} )-(Y_i (0)-\mu_{Y(0)} )] D_i\\
&= a+bD_i+u_i
\end{align*}

This equation implies that $b=\mu_{Y(1)}-\mu_{Y(0)}$, which is the ATE because the expected value of $Y_i (1)$ is $\mu_{Y(1)}$, and the expected value of $Y_i (0)$ is $\mu_{Y(0)}$.

\end{enumerate}

\section*{Question 2}

A researcher working with Israeli elementary school students sought to improve students' ability to solve logic puzzles.\footnote{Dan Gendelman conducted this study in 2004 and shared it with us via personal communication.} Students in the treatment and control group initially took a computer-administered test, and the number of correctly solved puzzles was recorded. A few days later, students assigned to the control group were then given 30 minutes to improve their puzzle-solving skills by playing on a computer. During the same allotment of time, students in the treatment group listened to an instructor describe some rules of thumb to keep in mind when solving logic puzzles. All subjects then took a computer-administered post-test, and the number of correctly solved puzzles was recorded. The table below shows the results for each subject.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Question 2 Table}
    \begin{tabular}{rcccc}
    \toprule
    Subject & D     & Pre-test & Post-test & Improvement \\
    \midrule
    1     & 1     & 10    & 10    & 0 \\
    2     & 1     & 9     & 11    & 2 \\
    3     & 1     & 5     & 6     & 1 \\
    4     & 1     & 3     & 6     & 3 \\
    5     & 1     & 3     & 6     & 3 \\
    6     & 1     & 6     & 7     & 1 \\
    7     & 1     & 6     & 7     & 1 \\
    8     & 1     & 5     & 6     & 1 \\
    9     & 1     & 6     & 7     & 1 \\
    10    & 0     & 9     & 9     & 0 \\
    11    & 0     & 6     & 7     & 1 \\
    12    & 0     & 11    & 10    & -1 \\
    13    & 0     & 4     & 5     & 1 \\
    14    & 0     & 3     & 3     & 0 \\
    15    & 0     & 10    & 10    & 0 \\
    16    & 0     & 7     & 8     & 1 \\
    17    & 0     & 7     & 7     & 0 \\
    18    & 0     & 8     & 10    & 2 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}

\begin{enumerate}[a)]
\item As a randomization check, use randomization inference to test the null hypothesis that the pre-test scores are unaffected by treatment assignment.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{D} \hlkwb{<-} \hlstd{rush}\hlopt{$}\hlstd{treat}
\hlstd{Y} \hlkwb{<-} \hlstd{rush}\hlopt{$}\hlstd{posttest}
\hlstd{X} \hlkwb{<-} \hlstd{rush}\hlopt{$}\hlstd{pretest}

\hlcom{# RI: randomization check, testing the effect of the covariate on the treatment}

\hlstd{perms} \hlkwb{<-} \hlkwd{genperms}\hlstd{(D,}\hlkwc{maxiter}\hlstd{=}\hlnum{10000}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Too many permutations to use exact method.
## Defaulting to approximate method.
## Increase maxiter to at least 48620 to perform exact estimation.
\end{verbatim}
\begin{alltt}
\hlstd{numiter} \hlkwb{<-} \hlkwd{ncol}\hlstd{(perms)}
\hlstd{Fstat} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{lm}\hlstd{(D}\hlopt{~}\hlstd{X))}\hlopt{$}\hlstd{fstatistic[}\hlnum{1}\hlstd{]}

\hlstd{Fstatstore} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,numiter)}

\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{numiter) \{}
  \hlstd{Fstatstore[i]} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{lm}\hlstd{(perms[,i]}\hlopt{~}\hlstd{X))}\hlopt{$}\hlstd{fstatistic[}\hlnum{1}\hlstd{]}
        \hlstd{\}}

\hlstd{p.value} \hlkwb{<-} \hlkwd{mean}\hlstd{(Fstatstore} \hlopt{>=} \hlstd{Fstat)}
\hlstd{p.value}
\end{alltt}
\begin{verbatim}
## [1] 0.2627
\end{verbatim}
\end{kframe}
\end{knitrout}

We calculated the F-statistic of a regression of treatment assignment on the pretest score for all possible randomizations, and found that the observed F-statistic was larger than 26.27\% of the simulated statistics, implying a $p$-value of 0.263. As expected,we fail to reject the null hypothesis that the treatment assignment is unrelated to the pretreatment covariate, pretest.

\item Use difference-in-means estimation to estimate the effect of the treatment on the post-test score.  Form a 95\% confidence interval.\\

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{probs} \hlkwb{<-} \hlkwd{genprobexact}\hlstd{(D)}
\hlstd{ate} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{ate}
\end{alltt}
\begin{verbatim}
## [1] -0.3333
\end{verbatim}
\begin{alltt}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=ate)}
\hlstd{distout} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{ci.95} \hlkwb{<-} \hlkwd{quantile}\hlstd{(distout,}\hlkwc{probs}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.975}\hlstd{))}
\hlstd{ci.95}
\end{alltt}
\begin{verbatim}
##   2.5%  97.5% 
## -2.333  1.593
\end{verbatim}
\end{kframe}
\end{knitrout}

We obtained a difference-in-means estimate of the ATE of \ensuremath{-0.3333333} and a 95\% confidence interval of [\ensuremath{-2.33}, 1.59]. This confidence interval is wide enough to include much larger and much smaller treatment effects -- even crossing zero.

\item Use difference-in-differences estimation to estimate the effect of the treatment on the post-test score. Form a 95\% confidence interval, and compare it to the interval in part (b).

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Y.improve} \hlkwb{<-} \hlstd{rush}\hlopt{$}\hlstd{improvement}
\hlstd{ate.improve} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y.improve,D,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{ate.improve}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\begin{alltt}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y.improve,D,}\hlkwc{ate}\hlstd{=ate.improve)}
\hlstd{distout} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{ci.95.improve} \hlkwb{<-} \hlkwd{quantile}\hlstd{(distout,}\hlkwc{probs}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.975}\hlstd{))}
\hlstd{ci.95.improve}
\end{alltt}
\begin{verbatim}
##   2.5%  97.5% 
## 0.1111 1.8889
\end{verbatim}
\end{kframe}
\end{knitrout}

By subtracting a pre-test, we have sharpened our estimates.  The difference-in-differences estimate of the ATE is 1 and the 95\% confidence interval is [0.11, 1.89].  No longer does the 95\% confidence interval cross zero, meaning we can be confidence at the 95\% level that the estimated ATE is larger than zero. This contrasts with part b) where the background variability in test scores made the estimation of a small treatment effect more difficult.

\end{enumerate}

\section*{Question 3}
The table below illustrates the problems that may arise when researchers exercise discretion over what results to report to readers. Suppose the true ATE associated with a given treatment were 1.0. The table reports the estimated ATE from nine experiments, each of which involves approximately 200 subjects. Each study produces two estimates, one based on a difference-in-means and another using regression to control for covariates. In principle, both estimators generate unbiased estimates, and covariate adjustment has a slight edge in terms of precision. Suppose the researchers conducting each study use the following decision rule: ``Estimate the ATE using both estimators and report whichever estimate is larger.'' Under this reporting policy, are the reported estimates unbiased? Why or why not?\\
Answer:\\
This procedure leads to biased estimates. Although each estimator is unbiased, the greater of two unbiased estimates is not unbiased. One can think of this procedure as ``Report the no-covariates estimate unless the with-covariates estimate is larger, in which case report the with-covariates estimate.'' On its own, the no-covariates estimate is unbiased, but it tends to be corrected when it generates a lower-than-average estimate. In this example, the average estimate generated by this reporting procedure is 12/9 = 1.33, which is greater than the true ATE of 1.0.

\begin{table}[H]
  \centering
  \caption{Question 3 table}
    \begin{tabular}{r|cc|c}
    \toprule
   \multicolumn{1}{r}{Study} & No covariates & \multicolumn{1}{c}{With covariates} & \multicolumn{1}{c}{Greater of two estimates} \\
    \midrule
    1     & 5     & 4     & 5 \\
    2     & 3     & 3     & 3 \\
    3     & 2     & 2     & 2 \\
    4     & 6     & 5     & 6 \\
    5     & 1     & 1     & 1 \\
    6     & 0     & 0     & 0 \\
    7     & -3    & -1    & -1 \\
    8     & -5    & -4    & -4 \\
    9     & 0     & -1    & 0 \\ \midrule
    Average & 1     & 1     & 1.33 \\
    Standard Deviation & 3.54  & 2.83  & 3.08 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

\section*{Question 4}


Table 4.1 contains a column of treatment assignments, $D_i$, that reflects a complete random assignment of 20 schools to treatment and 20 schools to control.

\begin{enumerate}[a)]
\item Use equation (2.2) to generate observed outcomes based on these assigned treatments.  Regress $Y_i$ on $D_i$ and interpret the slope and intercept.  Is the estimated slope the same as the estimated ATE based on a difference-in-means?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{D}  \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{D}
\hlstd{Y1} \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{y1}
\hlstd{Y0} \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{y0}
\hlstd{X} \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{x}

\hlstd{Y} \hlkwb{<-} \hlstd{Y0}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{D)} \hlopt{+} \hlstd{Y1}\hlopt{*}\hlstd{(D)}    \hlcom{# Equation 2.2}

\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(Y}\hlopt{~}\hlstd{D)}
\hlstd{arm::}\hlkwd{display}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## lm(formula = Y ~ D)
##             coef.est coef.se
## (Intercept) 26.85     3.33  
## D           10.70     4.71  
## ---
## n = 40, k = 2
## residual sd = 14.89, R-Squared = 0.12
\end{verbatim}
\begin{alltt}
\hlstd{diff_means} \hlkwb{<-} \hlkwd{mean}\hlstd{(Y[D}\hlopt{==}\hlnum{1}\hlstd{])}\hlopt{-}\hlkwd{mean}\hlstd{(Y[D}\hlopt{==}\hlnum{0}\hlstd{])}
\hlstd{diff_means}
\end{alltt}
\begin{verbatim}
## [1] 10.7
\end{verbatim}
\end{kframe}
\end{knitrout}

The estimate obtained with OLS regression (10.7) is identical to the estimate obtained with difference-in-means (10.7).
\item Regress treated and untreated outcomes on $X_i$ to see whether the condition in equation (4.6) appears to hold.  What do you infer about the advisability of rescaling the dependent variable so that the outcome is a change (i.e., $Y_i-X_i$)?\\
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit.1} \hlkwb{<-} \hlkwd{lm}\hlstd{(Y}\hlopt{~}\hlstd{X,}\hlkwc{subset}\hlstd{=D}\hlopt{==}\hlnum{1}\hlstd{)}
\hlstd{fit.0} \hlkwb{<-} \hlkwd{lm}\hlstd{(Y}\hlopt{~}\hlstd{X,}\hlkwc{subset}\hlstd{=D}\hlopt{==}\hlnum{0}\hlstd{)}

\hlstd{sum_of_coefficients} \hlkwb{<-} \hlstd{fit.1}\hlopt{$}\hlstd{coefficients[}\hlnum{2}\hlstd{]} \hlopt{+} \hlstd{fit.0}\hlopt{$}\hlstd{coefficients[}\hlnum{2}\hlstd{]}
\hlstd{sum_of_coefficients}
\end{alltt}
\begin{verbatim}
##     X 
## 1.846
\end{verbatim}
\begin{alltt}
\hlstd{Ydiff} \hlkwb{<-} \hlstd{Y}\hlopt{-}\hlstd{X}
\hlstd{fit.diff} \hlkwb{<-} \hlkwd{lm}\hlstd{(Ydiff}\hlopt{~}\hlstd{D)}
\hlstd{arm::}\hlkwd{display}\hlstd{(fit.diff)}
\end{alltt}
\begin{verbatim}
## lm(formula = Ydiff ~ D)
##             coef.est coef.se
## (Intercept) -1.00     1.15  
## D            4.85     1.62  
## ---
## n = 40, k = 2
## residual sd = 5.13, R-Squared = 0.19
\end{verbatim}
\end{kframe}
\end{knitrout}

Substituting regression estimates for the true ratio of covariances to variances satisfies the inequality, suggesting that the use of this covariate will improve precision.

\begin{align*}
\widehat{\frac{Cov(Y_i(0), X_i)}{Var(X_i)}} + \widehat{\frac{Cov(Y_i(1), X_i)}{Var(X_i)}} = 0.8995221 + 0.9465386 = 1.8460607
\end{align*}

We also see that the standard errors have shrunk substantially -- the standard error for a regression of $Y$ on $D$ is 4.7093133, whereas the standard error for the regression of the change in $Y$ on $D$ is 1.6226603

\item   Regress $Y_i$ on $D_i$ and $X_i$. Interpret the regression coefficients, contrasting these results with those obtained from a regression of $Y_i$ on $D_i$ alone.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit.cov} \hlkwb{<-} \hlkwd{lm}\hlstd{(Y}\hlopt{~}\hlstd{D}\hlopt{+}\hlstd{X)}
\hlstd{arm::}\hlkwd{display}\hlstd{(fit.cov)}
\end{alltt}
\begin{verbatim}
## lm(formula = Y ~ D + X)
##             coef.est coef.se
## (Intercept) 1.22     1.88   
## D           5.32     1.63   
## X           0.92     0.05   
## ---
## n = 40, k = 3
## residual sd = 5.05, R-Squared = 0.90
\end{verbatim}
\end{kframe}
\end{knitrout}

The estimated ATE (5.32) is now roughly half the size as the original difference-in-means.  (This estimate also happens to be much closer to the true ATE of 4.0.) Comparing the estimated standard errors from both regressions suggests that the inclusion of a covariate has greatly improved precision.

\item With the estimates obtained in part (a), use randomization inference (as described in Chapter 3) to evaluate the sharp null hypothesis of no effect for any school. To obtain the sampling distribution under the sharp null hypothesis, simulate 100,000 random assignments, and for each simulated sample, estimate the ATE using a regression of $Y_i$ on $d_i$. Interpret the results.  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{perms} \hlkwb{<-} \hlkwd{genperms}\hlstd{(D,}\hlkwc{maxiter}\hlstd{=}\hlnum{10000}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Too many permutations to use exact method.
## Defaulting to approximate method.
## Increase maxiter to at least 137846528820 to perform exact estimation.
\end{verbatim}
\begin{alltt}
\hlstd{probs} \hlkwb{<-} \hlkwd{genprobexact}\hlstd{(D)}
\hlstd{ate} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=}\hlnum{0}\hlstd{)}

\hlstd{distout} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{p.value} \hlkwb{<-} \hlkwd{mean}\hlstd{(}\hlkwd{abs}\hlstd{(distout)}\hlopt{>}\hlkwd{abs}\hlstd{(ate))}

\hlstd{ate}
\end{alltt}
\begin{verbatim}
## [1] 10.7
\end{verbatim}
\begin{alltt}
\hlstd{p.value}
\end{alltt}
\begin{verbatim}
## [1] 0.0291
\end{verbatim}
\begin{alltt}
\hlkwd{hist}\hlstd{(distout,} \hlkwc{breaks}\hlstd{=}\hlnum{1000}\hlstd{,}
     \hlkwc{main}\hlstd{=}\hlstr{"Q4d: Distribution of the Estimated ATE"}\hlstd{,}
     \hlkwc{xlab}\hlstd{=}\hlstr{"Estimated ATE"}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwc{v}\hlstd{=ate,} \hlkwc{col}\hlstd{=}\hlstr{"red"}\hlstd{,} \hlkwc{lty}\hlstd{=}\hlnum{3}\hlstd{,} \hlkwc{lwd}\hlstd{=}\hlnum{3}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=4in,height=4in]{figure/PS4-PS4-4-d-1} 

}



\end{knitrout}
We use a two-tailed test in order to evaluate the null hypothesis that the treatment has no effect for any subject.  We find a two-tailed $p$-value of 0.029, which leads us to reject the null hypothesis in favor of the alternative hypothesis that the treatment has some positive effect.

\item Using the estimator in part (c), use randomization inference to evaluate the sharp null hypothesis of no effect for any school. To obtain the sampling distribution under the sharp null hypothesis, simulate 100,000 random assignments, and for each simulated sample, estimate the ATE using a regression of $Y_i$ on $D_i$ and $X_i$.  Interpret the results.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ate_cov} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,X,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{distout_cov} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,X,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{p.value_cov} \hlkwb{<-} \hlkwd{mean}\hlstd{(}\hlkwd{abs}\hlstd{(distout_cov)}\hlopt{>}\hlkwd{abs}\hlstd{(ate_cov))}
\hlstd{ate_cov}
\end{alltt}
\begin{verbatim}
##     Z 
## 5.316
\end{verbatim}
\begin{alltt}
\hlstd{p.value_cov}
\end{alltt}
\begin{verbatim}
## [1] 0.0026
\end{verbatim}
\end{kframe}
\end{knitrout}

We again use a two-tailed test in order to evaluate the null hypothesis that the treatment has no effect for any subject.  We find a two-tailed p-value of 0.003, which leads us to reject the null hypothesis in favor of the alternative hypothesis that the treatment has some effect.

\item Use the estimated ATE in part (a) to construct a full schedule of potential outcomes for all schools, assuming that every school has the same treatment effect. Using this simulated schedule of potential outcomes, construct a 95\% confidence interval for the sample average treatment effect in the following way. First, assign each subject to treatment or control, and estimate the ATE by a regression of $Y_i$ on $D_i$. Repeat this procedure until you have 100,000 estimates of the ATE.  Order the estimates from smallest to largest. The 2,501st estimate marks the 2.5th percentile, and the 97,500th estimate marks the 97.5th percentile. Interpret the results.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=ate)}
\hlstd{distout} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{ci.95} \hlkwb{<-} \hlkwd{quantile}\hlstd{(distout,} \hlkwc{probs}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{.975}\hlstd{))}
\hlstd{ci.95}
\end{alltt}
\begin{verbatim}
##  2.5% 97.5% 
##  1.53 19.84
\end{verbatim}
\end{kframe}
\end{knitrout}

The confidence interval stretches from [1.53, 19.84] implying that the ATE is positive but its location is subject to a great deal of statistical uncertainty.  Our best guess is 10.7, but the interval ranges from a small positive value to a truly massive effect.

\item Use the estimated ATE in part (c) to construct a full schedule of potential outcomes for all schools, assuming that every school has the same treatment effect.  Using this simulated schedule of potential outcomes, simulate the 95\% confidence interval for the sample average treatment effect estimated by a regression of $Y_i$ on $D_i$ and $X_i$.  Interpret the results.  Is this confidence interval narrower than one you generated in question (f)?  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Ys_cov} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=ate_cov)}
\hlstd{distout_cov} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys_cov,perms,X,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{ci.95_cov} \hlkwb{<-} \hlkwd{quantile}\hlstd{(distout_cov,} \hlkwc{probs}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{.975}\hlstd{))}
\hlstd{ci.95_cov}
\end{alltt}
\begin{verbatim}
##  2.5% 97.5% 
## 2.225 8.442
\end{verbatim}
\end{kframe}
\end{knitrout}

The confidence interval now stretches from [2.23, 8.44].  Interestingly, this interval no longer contains the estimate obtained without controls for covariates.  Our best guess is now 5.32, and our 95\% interval is now roughly one-third as wide as before.

\end{enumerate}

\section*{Question 5}


Randomizations are said to be ``restricted'' when the set of all possible random allocations is narrowed to exclude allocations that have inadequate covariate balance.  Suppose, for example, that the assignment of treatments ($D_i$) in Table 4.1 was conducted subject to the restriction that a regression of $D_i$ on $X_i$ (the pretest) does not allow the researcher to reject the sharp null hypothesis of no effect of $X_i$ on $D_i$ at the 0.05 significance level) produces a $p$-value on that is greater than 0.05. In other words, had the researcher found that the assigned $D_i$ were significantly predicted by $X_i$, the random allocation would have been conducted again, until the $D_i$ met this criterion.  

\begin{enumerate}[a)]
\item Conduct a series of random assignments in order to calculate the weighting variable $w_i$; for units in the treatment group, this weight is defined as the inverse of the probability of being assigned to treatment, and for units in the control group, this weight is defined as the inverse of the probability of being assigned to control. See Table 4.2 for an example. Does $w_i$ appear to vary within the treatment group or within the control group?

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{D}  \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{D}
\hlstd{Y1} \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{y1}
\hlstd{Y0} \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{y0}
\hlstd{X} \hlkwb{<-} \hlstd{teach}\hlopt{$}\hlstd{x}

\hlstd{Y} \hlkwb{<-} \hlstd{Y0}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{D)} \hlopt{+} \hlstd{Y1}\hlopt{*}\hlstd{(D)}
\hlstd{N} \hlkwb{<-} \hlkwd{length}\hlstd{(D)}


\hlstd{randfun} \hlkwb{<-} \hlkwa{function}\hlstd{() \{}
  \hlstd{teststat} \hlkwb{<-} \hlopt{-}\hlnum{1}
  \hlkwa{while} \hlstd{(teststat} \hlopt{<} \hlnum{0.05}\hlstd{) \{}
                \hlstd{Zri} \hlkwb{<-} \hlkwd{sample}\hlstd{(D)}
                \hlstd{teststat} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{lm}\hlstd{(Zri}\hlopt{~}\hlstd{X))}\hlopt{$}\hlstd{coefficients[}\hlnum{2}\hlstd{,}\hlnum{4}\hlstd{]}
        \hlstd{\}}
        \hlkwd{return}\hlstd{(Zri)}
\hlstd{\}}

\hlcom{# notice the use of the restricted randomization function.}
\hlcom{# restricted randomization often generates unequal probabilities of assignment. }
\hlcom{# if so, inverse probability weighting is required.}

\hlstd{perms} \hlkwb{<-} \hlkwd{genperms.custom}\hlstd{(}\hlkwc{numiter}\hlstd{=}\hlnum{10000}\hlstd{,}\hlkwc{randfun}\hlstd{=randfun)}
\hlstd{probs} \hlkwb{<-} \hlkwd{genprob}\hlstd{(perms)}
\hlstd{weights} \hlkwb{<-} \hlstd{(}\hlnum{1}\hlopt{/}\hlstd{probs)} \hlopt{*}\hlstd{D} \hlopt{+} \hlstd{(}\hlnum{1}\hlopt{/}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{probs))}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{D)}
\hlstd{var.weights.treat} \hlkwb{<-} \hlkwd{var}\hlstd{(weights[D}\hlopt{==}\hlnum{1}\hlstd{])}
\hlstd{var.weights.control} \hlkwb{<-} \hlkwd{var}\hlstd{(weights[D}\hlopt{==}\hlnum{0}\hlstd{])}
\end{alltt}
\end{kframe}
\end{knitrout}

The variance of the weights is \ensuremath{4\times 10^{-4}} in the treatment condition and \ensuremath{6\times 10^{-4}} in the control condition. Indeed, units do have different probabilities of assignments as a result of the restriction scheme, but the differences are small.

\item Use randomization inference to test the sharp null hypothesis that $D_i$ has no effect on $Y_i$ by regressing $Y_i$ on $D_i$ and comparing the estimate to the sampling distribution under the null hypothesis. Make sure that your sampling distribution includes only random allocations that satisfy the restriction mentioned above. Be sure to weight units by inverse probability weights as produced by the random allocation procedure. Estimate the ATE, calculate the $p$-value, and interpret the results.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ate} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=}\hlnum{0}\hlstd{)}
\hlstd{distout} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{p.value} \hlkwb{<-} \hlkwd{mean}\hlstd{(}\hlkwd{abs}\hlstd{(distout)} \hlopt{>} \hlkwd{abs}\hlstd{(ate))}
\hlstd{ate}
\end{alltt}
\begin{verbatim}
## [1] 10.73
\end{verbatim}
\begin{alltt}
\hlstd{p.value}
\end{alltt}
\begin{verbatim}
## [1] 0.0054
\end{verbatim}
\end{kframe}
\end{knitrout}


The IPW estimate of the ATE is 10.73, which is close to the unweighted estimate above.  Using a two-tailed test in order to evaluate the null hypothesis that the treatment has no effect for any subject, we find a p-value of 0.005, which leads us to reject the null hypothesis in favor of the alternative hypothesis that the treatment has some effect. 

\item Use randomization inference to test the sharp null hypothesis that $D_i$ has no effect on $Y_i$ by regressing $Y_i$ on $D_i$ and $X_i$ and comparing the estimate to the sampling distribution under the null hypothesis.  Estimate the ATE, calculate the $p$-value, and interpret the results. 


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{perms} \hlkwb{<-} \hlkwd{genperms.custom}\hlstd{(}\hlkwc{numiter}\hlstd{=}\hlnum{10000}\hlstd{,}\hlkwc{randfun}\hlstd{=randfun)}
\hlstd{probs} \hlkwb{<-} \hlkwd{genprob}\hlstd{(perms)}
\hlstd{ate_cov} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,X,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=}\hlnum{0}\hlstd{)}
\hlstd{distout_cov} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,X,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{p.value_cov} \hlkwb{<-} \hlkwd{mean}\hlstd{(}\hlkwd{abs}\hlstd{(distout_cov)} \hlopt{>} \hlkwd{abs}\hlstd{(ate_cov))}
\hlstd{ate_cov}
\end{alltt}
\begin{verbatim}
##     Z 
## 5.346
\end{verbatim}
\begin{alltt}
\hlstd{p.value_cov}
\end{alltt}
\begin{verbatim}
## [1] 0.0017
\end{verbatim}
\end{kframe}
\end{knitrout}

The IPW estimate of the ATE is 5.35, which is close to the unweighted estimate above. We again use a two-tailed test in order to evaluate the null hypothesis that the treatment has no effect for any subject. We find a $p$-value of 0.002, which leads us to reject the null hypothesis in favor of the alternative hypothesis that the treatment has some effect.

\item Compare the sampling distributions under the null hypothesis in parts (a) and (b) to the sampling distributions obtained in exercises 4(d) and 4(e), which assumed that the randomization was unrestricted.  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## Sampling Distributions from 4(d) and 4(e)}
\hlstd{perms_complete_RA} \hlkwb{<-} \hlkwd{genperms}\hlstd{(D,}\hlkwc{maxiter}\hlstd{=}\hlnum{10000}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Too many permutations to use exact method.
## Defaulting to approximate method.
## Increase maxiter to at least 137846528820 to perform exact estimation.
\end{verbatim}
\begin{alltt}
\hlstd{probs_complete_RA} \hlkwb{<-} \hlkwd{genprobexact}\hlstd{(D)}

\hlstd{ate_complete_RA} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,}\hlkwc{prob}\hlstd{=probs_complete_RA)}
\hlstd{Ys_complete_RA} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=ate_complete_RA)}
\hlstd{distout_complete_RA} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys_complete_RA,perms_complete_RA,}
                               \hlkwc{prob}\hlstd{=probs_complete_RA)}
\hlstd{se_complete_RA} \hlkwb{<-} \hlkwd{sd}\hlstd{(distout_complete_RA)}
\hlstd{se_complete_RA}
\end{alltt}
\begin{verbatim}
## [1] 4.601
\end{verbatim}
\begin{alltt}
\hlstd{ate_cov_complete_RA} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,X,}\hlkwc{prob}\hlstd{=probs_complete_RA)}
\hlstd{Ys_cov_complete_RA} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=ate_cov_complete_RA)}
\hlstd{distout_cov_complete_RA} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys_cov_complete_RA,perms_complete_RA,X,}
                                   \hlkwc{prob}\hlstd{=probs_complete_RA)}
\hlstd{se_cov_complete_RA} \hlkwb{<-} \hlkwd{sd}\hlstd{(distout_cov_complete_RA)}
\hlstd{se_cov_complete_RA}
\end{alltt}
\begin{verbatim}
## [1] 1.593
\end{verbatim}
\begin{alltt}
\hlcom{## Sampling Distributions from 5(a) and 5(b)}
\hlstd{perms_restricted_RA} \hlkwb{<-} \hlkwd{genperms.custom}\hlstd{(}\hlkwc{numiter}\hlstd{=}\hlnum{10000}\hlstd{,}\hlkwc{randfun}\hlstd{=randfun)}
\hlstd{probs_restricted_RA} \hlkwb{<-} \hlkwd{genprob}\hlstd{(perms_restricted_RA)}

\hlstd{ate_restricted_RA} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,}\hlkwc{prob}\hlstd{=probs_restricted_RA)}
\hlstd{Ys_restricted_RA} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=ate_restricted_RA)}
\hlstd{distout_restricted_RA} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys_restricted_RA,perms_restricted_RA,}
                                 \hlkwc{prob}\hlstd{=probs_restricted_RA)}
\hlstd{se_restricted_RA} \hlkwb{<-} \hlkwd{sd}\hlstd{(distout_restricted_RA)}
\hlstd{se_restricted_RA}
\end{alltt}
\begin{verbatim}
## [1] 4.199
\end{verbatim}
\begin{alltt}
\hlstd{ate_cov_restricted_RA} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,D,X,}\hlkwc{prob}\hlstd{=probs_restricted_RA)}
\hlstd{Ys_cov_restricted_RA} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,D,}\hlkwc{ate}\hlstd{=ate_cov_restricted_RA)}
\hlstd{distout_cov_restricted_RA} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys_cov_restricted_RA,perms_restricted_RA,X,}
                                     \hlkwc{prob}\hlstd{=probs_restricted_RA)}
\hlstd{se_cov_restricted_RA} \hlkwb{<-} \hlkwd{sd}\hlstd{(distout_cov_restricted_RA)}
\hlstd{se_cov_restricted_RA}
\end{alltt}
\begin{verbatim}
## [1] 1.607
\end{verbatim}
\end{kframe}
\end{knitrout}

% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Fri Feb 27 19:40:43 2015
\begin{table}[H]
\centering
\caption{Summary of Estimated Standard Errors} 
\begin{tabular}{rrr}
  \hline
 & Without Covariates & With Covariates \\ 
  \hline
Complete Random Assignment & 4.601 & 1.593 \\ 
  Restricted Random Assignment & 4.199 & 1.607 \\ 
   \hline
\end{tabular}
\end{table}


Without covariates and assuming complete randomization, we obtain a standard error of 4.601. Under restricted randomization, the standard error declines to 4.199.  Including a covariate and assuming complete randomization, we obtain a standard error of 1.593.  Under restricted randomization, the standard error remains essentially unchanged at 1.607. Restricted randomization is akin to blocking, in that it rules out random allocations that result in imbalance; however, its advantages in terms of precision are limited when the researcher controls for a strongly prognostic covariate, which achieves most of the precision gains associated with blocking. 

\end{enumerate}

\section*{Question 6}

One way to practice your experimental design skills is to undertake a mock randomization of an existing non-experimental dataset. In this exercise, the existing dataset is treated as though it were a baseline data collection effort that an experimental researcher gathered in preparation for a random intervention. The actual data in question come from a panel study of Russian villagers. Villagers from randomly selected rural areas of Russia were interviewed in 1995 and re-interviewed in 1996 and 1997.  Our attention focuses on the 462 respondents who were interviewed in all three waves and provided answers to questions about their income, church membership, and evaluation of national conditions (i.e., how well are things going in Russia?). Imagine that an experimental intervention occurred after the 1996 survey and that national evaluations in the 1997 survey were the experimental outcome of interest.  The dataset provided at isps.research.yale.due/FEDAI contains the following pre-treatment covariates that may be used for blocking: sex, church membership, social class, and evaluations of national conditions in 1995 and 1996. As you design your experiment, imagine that ``post-intervention'' evaluations of national conditions in 1997 were unknown.

\begin{enumerate}[a)]
\item One way to develop a sense of which variables are likely to predict post-intervention evaluations of national conditions in 1997 is to regress evaluations of national conditions in 1996 on sex, church membership, social class, and evaluations in 1995. Which of these variables seem to most strongly predict evaluations of national conditions in 1996?  What is the $R^2$ from this regression? 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{russia} \hlkwb{<-} \hlkwd{within}\hlstd{(russia,\{}
  \hlstd{female} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(sexresp6} \hlopt{==} \hlstr{"woman"}\hlstd{)}
  \hlstd{class} \hlkwb{<-} \hlkwd{relevel}\hlstd{(group6,}\hlkwc{ref}\hlstd{=}\hlstr{"very poor"}\hlstd{)}
  \hlstd{church_member} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(memberc6}\hlopt{==}\hlstr{"yes"}\hlstd{)}
  \hlstd{id} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{nrow}\hlstd{(russia)}
  \hlstd{class_verypoor} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(class}\hlopt{==}\hlstr{"very poor"}\hlstd{)}
  \hlstd{class_poor} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(class}\hlopt{==}\hlstr{"poor"}\hlstd{)}
  \hlstd{class_middle} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(class}\hlopt{==}\hlstr{"middle"}\hlstd{)}
  \hlstd{class_morethanmiddle} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(class}\hlopt{==}\hlstr{"more than middle"}\hlstd{)}
\hlstd{\})}

\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(index96} \hlopt{~} \hlstd{index95} \hlopt{+} \hlstd{female} \hlopt{+} \hlstd{church_member} \hlopt{+} \hlstd{class,} \hlkwc{data}\hlstd{=russia)}
\hlkwd{summary}\hlstd{(fit)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.3937
\end{verbatim}
\begin{alltt}
\hlstd{fit.nolag} \hlkwb{<-} \hlkwd{lm}\hlstd{(index96} \hlopt{~} \hlstd{female} \hlopt{+} \hlstd{church_member} \hlopt{+} \hlstd{class,} \hlkwc{data}\hlstd{=russia)}
\hlkwd{summary}\hlstd{(fit.nolag)}\hlopt{$}\hlstd{r.squared}
\end{alltt}
\begin{verbatim}
## [1] 0.02828
\end{verbatim}
\end{kframe}
\end{knitrout}

The regression treats ``index95'' as a continuous variable and all others as categorical.  the R-squared is 0.394, which implies that the regressors predict about 40\% of the variance in ``index96''.  The strongest predictor is 95, the lagged dependent variable.  Had we omitted this variable from the model, the R-squared would have fallen to 0.028.

\item Suppose you were to design a block random assignment in order to predict evaluations in 1997. Use the R package \texttt{blockTools} (for example code, see isps.research.yale.due/FEDAI) to perform a block random assignment, blocking on sex, church membership, social class, and evaluations in 1996. Decide for yourself how many subjects to include in each block. Compare the treatment and control groups to verify that blocking produced groups that have the same profile of sex, church membership, social class, and evaluations in 1996.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{block.out} \hlkwb{<-} \hlkwd{block}\hlstd{(}\hlkwc{data} \hlstd{= russia,} \hlkwc{n.tr} \hlstd{=} \hlnum{2}\hlstd{,}
                   \hlkwc{id.vars} \hlstd{=} \hlstr{"id"}\hlstd{,}\hlkwc{algorithm}\hlstd{=}\hlstr{"randGreedy"}\hlstd{,}
                    \hlkwc{block.vars} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"female"}\hlstd{,} \hlstr{"church_member"}\hlstd{,}
                                   \hlstr{"index96"}\hlstd{,}\hlstr{"class_verypoor"}\hlstd{,}
                                   \hlstr{"class_poor"}\hlstd{,} \hlstr{"class_middle"}\hlstd{))}
\hlstd{assign.out} \hlkwb{<-} \hlkwd{assignment}\hlstd{(block.out)}

\hlcom{# extracting the treatment assignment from blockTools takes some work}
\hlcom{# The commands below check to see which ID numbers appear on the }
\hlcom{# list of assign.out's assignment to Treatment 1}

\hlstd{russia}\hlopt{$}\hlstd{Z_blocked} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(russia}\hlopt{$}\hlstd{id} \hlopt{%in%}
                         \hlkwd{as.numeric}\hlstd{(}\hlkwd{as.character}\hlstd{(}
                           \hlkwd{unlist}\hlstd{(assign.out}\hlopt{$}\hlstd{assg[[}\hlnum{1}\hlstd{]][}\hlstr{"Treatment 1"}\hlstd{]))))}

\hlstd{arm}\hlopt{::}\hlkwd{display}\hlstd{(}\hlkwd{lm}\hlstd{(Z_blocked} \hlopt{~} \hlstd{female} \hlopt{+} \hlstd{church_member} \hlopt{+} \hlstd{class} \hlopt{+} \hlstd{index96,}
                \hlkwc{data}\hlstd{=russia))}
\end{alltt}
\begin{verbatim}
## lm(formula = Z_blocked ~ female + church_member + class + index96, 
##     data = russia)
##                       coef.est coef.se
## (Intercept)            0.53     0.17  
## female                 0.00     0.06  
## church_member          0.01     0.08  
## classpoor             -0.04     0.16  
## classmiddle           -0.05     0.16  
## classmore than middle -0.05     0.22  
## index96                0.00     0.01  
## ---
## n = 462, k = 7
## residual sd = 0.50, R-Squared = 0.00
\end{verbatim}
\end{kframe}
\end{knitrout}

Using the package \texttt{blockTools}, we created blocks of size 2 based on gender, church membership, evaluations in 1996, and social class. The package also conducts complete random assignment -- with some work, this assignment can be extracted. Regressing this treatment assignment on the set of pretreatment covariates reveals that the groups are well balanced.

\item Suppose you wanted to assess how well your blocking design performed in terms of increasing the precision with which treatment effects are estimated. Of course, there was no actual treatment in this case, but imagine that shortly after the survey in 1996, a treatment were administered to a randomly selected treatment group. (Here is an instance in which the sharp null hypothesis of no effect is known to be true!) The outcome from this imaginary experiment is evaluations of national conditions in 1997. Compare the sampling distribution of the estimated treatment effect (which should be centered on zero) under balanced complete random assignment to the sampling distribution of the estimated treatment effect under block random assignment.\\
Answer:\\
See below

\item Calculate the sampling distribution of the estimated treatment effect under balanced complete random assignment using regression to control for the variables that would have otherwise been used to form blocks. Compare the resulting distribution to the sampling distribution of the estimated treatment effect under block random assignment.  Does blocking produce an appreciable gain in precision over what is achieved by covariate adjustment?  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sims} \hlkwb{<-} \hlnum{10000}
\hlstd{results} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{NA}\hlstd{,sims,}\hlnum{3}\hlstd{)}
\hlkwd{colnames}\hlstd{(results)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"complete"}\hlstd{,}\hlstr{"adjusted"}\hlstd{,}\hlstr{"blocked"}\hlstd{)}
\hlstd{N} \hlkwb{<-} \hlkwd{nrow}\hlstd{(russia)}

\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{sims) \{}
    \hlcom{# Complete RA, with and without adjustment}
    \hlstd{russia}\hlopt{$}\hlstd{Z_complete} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{N} \hlopt{%in%} \hlkwd{sample}\hlstd{(N, N}\hlopt{/}\hlnum{2}\hlstd{),} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{)}
    \hlstd{results[i,}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{lm}\hlstd{(index97} \hlopt{~} \hlstd{Z_complete,} \hlkwc{data}\hlstd{=russia)}\hlopt{$}\hlstd{coefficients[}\hlnum{2}\hlstd{]}
    \hlstd{results[i,}\hlnum{2}\hlstd{]} \hlkwb{<-} \hlkwd{lm}\hlstd{(index97} \hlopt{~} \hlstd{Z_complete} \hlopt{+} \hlstd{female} \hlopt{+} \hlstd{church_member} \hlopt{+} \hlstd{class} \hlopt{+} \hlstd{index96,}
                       \hlkwc{data}\hlstd{=russia)}\hlopt{$}\hlstd{coefficients[}\hlnum{2}\hlstd{]}

    \hlcom{# Blocked RA, without adjustment}
    \hlstd{assign.out} \hlkwb{<-} \hlkwd{assignment}\hlstd{(block.out)}
    \hlstd{russia}\hlopt{$}\hlstd{Z_blocked} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(russia}\hlopt{$}\hlstd{id} \hlopt{%in%}
                         \hlkwd{as.numeric}\hlstd{(}\hlkwd{as.character}\hlstd{(}
                           \hlkwd{unlist}\hlstd{(assign.out}\hlopt{$}\hlstd{assg[[}\hlnum{1}\hlstd{]][}\hlstr{"Treatment 1"}\hlstd{]))))}
    \hlstd{results[i,}\hlnum{3}\hlstd{]} \hlkwb{<-} \hlkwd{lm}\hlstd{(index97} \hlopt{~} \hlstd{Z_blocked,} \hlkwc{data}\hlstd{=russia)}\hlopt{$}\hlstd{coefficients[}\hlnum{2}\hlstd{]}
    \hlstd{\}}

\hlcom{# use apply() to extract means and SDs for each column (2 refers to columns)}
\hlstd{results_table} \hlkwb{<-} \hlkwd{rbind}\hlstd{(}\hlkwd{apply}\hlstd{(results,}\hlnum{2}\hlstd{,mean),}\hlkwd{apply}\hlstd{(results,}\hlnum{2}\hlstd{,sd))}
\hlkwd{rownames}\hlstd{(results_table)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Average Estimate"}\hlstd{,} \hlstr{"Standard Error"}\hlstd{)}
\hlstd{results_forxtable} \hlkwb{<-} \hlkwd{xtable}\hlstd{(results_table,}\hlkwc{caption}\hlstd{=}\hlstr{"Comparison of 3 estimators"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{kframe}
\begin{alltt}
\hlkwd{print.xtable}\hlstd{(results_forxtable,}\hlkwc{caption.placement}\hlstd{=}\hlstr{"top"}\hlstd{,}\hlkwc{table.placement}\hlstd{=}\hlstr{"H"}\hlstd{)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Fri Feb 27 19:43:56 2015
\begin{table}[H]
\centering
\caption{Comparison of 3 estimators} 
\begin{tabular}{rrrr}
  \hline
 & complete & adjusted & blocked \\ 
  \hline
Average Estimate & 0.00 & 0.00 & -0.00 \\ 
  Standard Error & 0.17 & 0.13 & 0.13 \\ 
   \hline
\end{tabular}
\end{table}


The table above shows a comparison of three estimators of the ATE: difference-in-means under complete random assignment, OLS with covariate adjustment under complete random assignment, and difference-in-means under blocked random assignment. All three estimators are centered on the true ATE of zero. The least precise method is complete random assignment with the difference-in-means estimator, which produces a standard error of 0.169.  The most precise approach is blocking, which produces a standard error of 0.131.  Slightly inferior to blocking is covariate adjustment, which produces a standard error of 0.133. Blocking's slight superiority stems from the fact that, under blocking, there is no incidental correlation between the covariates and random assignments and therefore no ``collinearity penalty.''

\end{enumerate}

\section*{Question 7}
Researchers may be concerned about using block randomization when they are unsure whether the variable used to form the blocks actually predicts the outcome. Consider the case in which blocks are formed randomly -- in other words, the variable used to form the blocks has no prognostic value whatsoever. Below is a schedule of potential outcomes for four observations.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Question 7 Table}
    \begin{tabular}{ccc}
    \toprule
    Subject & Y(0)  & Y(1) \\
    \midrule
    A     & 1     & 2 \\
    B     & 0     & 3 \\
    C     & 2     & 2 \\
    D     & 5     & 5 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

\begin{enumerate}[a)]
\item Suppose you were to use complete random assignment such that $m=2$ units are assigned to treatment.  What is the sampling variance of the difference-in-means estimator across all six possible random assignments?

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Question 7a table}
    \begin{tabular}{rrrr}
    \toprule
     Treated Units     &  $\bar{Y(1)}$ &  $\bar{Y(0)}$ &  $\widehat{ATE}$ \\
    \midrule
    A and B & 2.5   & 3.5   & -1 \\
    A and C & 2     & 2.5   & -0.5 \\
    A and D & 3.5   & 1     & 2.5 \\
    B and C & 2.5   & 3     & -0.5 \\
    B and D & 4     & 1.5   & 2.5 \\
    C and D & 3.5     & 0.5   & 3 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

The average estimated ATE is 1.0, which is the true ATE. The variance of the estimated ATEs over all 6 possible randomizations is 2.833.

\item Suppose you were to form blocks by randomly pairing the observations. Within each pair, you randomly allocate one subject to treatment and the other to control so that $m=2$ units are assigned to treatment.There are three possible blocking schemes; for each blocking scheme, there are four possible random assignments.  What is the sampling variance of the difference-in-means estimator across all twelve possible random assignments?

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Question 7b table}
    \begin{tabular}{rrrrr}
    \toprule
          & Treated Units     &  $\bar{Y(1)}$ &  $\bar{Y(0)}$ &  $\widehat{ATE}$ \\
    \midrule
    \multicolumn{1}{c}{\multirow{4}[0]{*}{AB and CD blocked}} & A,C   & 2     & 2.5   & -0.5 \\
    \multicolumn{1}{c}{} & A,D   & 3.5   & 1     & 2.5 \\
    \multicolumn{1}{c}{} & B,D   & 4     & 1.5   & 2.5 \\
    \multicolumn{1}{c}{} & B,C   & 2.5   & 3     & -0.5 \\
          &       &       &       &  \\
    \multicolumn{1}{c}{\multirow{4}[0]{*}{AC and BD blocked}} & A,B   & 2.5   & 3.5   & -1 \\
    \multicolumn{1}{c}{} & A,D   & 3.5   & 1     & 2.5 \\
    \multicolumn{1}{c}{} & C,B   & 2.5   & 3     & -0.5 \\
    \multicolumn{1}{c}{} & C,D   & 3.5   & 0.5   & 3 \\
          &       &       &       &  \\
    \multicolumn{1}{c}{\multirow{4}[0]{*}{AD and BC blocked}} & A,B   & 2.5   & 3.5   & -1 \\
    \multicolumn{1}{c}{} & A,C   & 2     & 2.5   & -0.5 \\
    \multicolumn{1}{c}{} & D,B   & 4     & 1.5   & 2.5 \\
    \multicolumn{1}{c}{} & D,C   & 3.5   & 0.5   & 3 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

Across the 12 possible random assignments, the variance of the estimated ATE is again 2.833.  Notice that every estimate in the previous table appears in this table twice.

\item From this example, what do you infer about the risks of blocking on a non-prognostic covariate?\\
Answer:\\
There is no risk of increasing variance with a useless blocking variable; at worst, the variable will be random noise, in which case the sampling variance will be the same as a design without blocking.
\end{enumerate}


\section*{Question 8}
Sometimes researchers randomly assign subjects from lists that are later discovered to have duplicate entries. Suppose, for example, that a fund-raising experiment randomly assigns 500 of 1,000 names to a treatment that consists of an invitation to contribute to a charitable cause.  However, it is later discovered that 600 names appear once and 200 names appear twice. Before the invitations are mailed, duplicate invitations are discarded, so that no one receives more than one invitation. 
\begin{enumerate}[a)]
\item What is the probability of assignment to the treatment group among those whose names appeared once in the original list?  What is the probability of assignment to the treatment group among those whose names appeared twice in the original list?\\
Answer:\\
The probability of being assigned to treatment if your name appears once is 0.5. The probability of being assigned to treatment if your name is a duplicate is 0.5 + (0.5)(0.5) = 0.75, where the first term is the probability you were assigned to treatment the first time your name came up and the second term is the probability you were assigned to control the first time multiplied by the probability you were assigned to treatment the second time.

\item Of the 800 unique names in the original list, how many would you expect to be assigned to treatment and control?  \\
Answer:\\
Of the 600 unique names that appear once, 300 are, in expectation, allocated to treatment. Of the 200 unique names that appear twice, 150 are, in expectation, allocated to treatment. Thus, we expect a total of 450 unique names in the treatment group.
\item What estimation procedure should one use in order to obtain unbiased estimates of the ATE?\\
Answer:\\
One should analyze the experiment as though it were randomized in two blocks: the names that appear once and the names that appear twice. Use an estimator like equation (4.11).
\end{enumerate}

\section*{Question 9}
Gerber and Green conducted a mobilization experiment in which calls from a large commercial phone bank urged voters in Iowa and Michigan to vote in the November 2002 election.\footnote{Gerber and Green 2005.} The randomization was conducted within four blocks: uncompetitive congressional districts in Iowa, competitive congressional districts in Iowa, uncompetitive congressional districts in Michigan, and competitive congressional districts in Michigan. Table 4.3 presents results only for one-voter households in order to sidestep the complications of cluster assignment.



\begin{enumerate}[a)]
\item Within each of the four blocks, what was the apparent effect of being called by a phone bank on voter turnout?  \\
Answer:\\
From the ``Estimated ATE'' Row: Block 1: .0096, Block 2: -.0078, Block 3: -.0136, Block 4: .0083.  Substantively, these results suggest that calls encouraging voter turnout had effects ranging from -1.4 percentage points to +1.0 percentage point.
\item When all of the subjects in this experiment are combined (see the rightmost column of the table), turnout seems substantially higher in the treatment group than the control group.  Explain why this comparison gives a biased estimate of the ATE.\\
Answer:\\
This estimator is biased because individuals in each stratum had different propensities to enter into treatment. The uncompetitive Michigan block has the lowest rate of treatment and also has the lowest rate of voting in the control group. Overall, blocks with higher rates of treatment tend to have higher rates of voting in the control group, which accounts for the upward bias.
\item Using the weighted estimator described in Chapter 3, show the calculations used to generate an unbiased estimate of the overall ATE. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ests} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{.00964}\hlstd{,} \hlopt{-}\hlnum{.007829}\hlstd{,} \hlopt{-}\hlnum{.01362}\hlstd{,}\hlnum{.008271}\hlstd{)}
\hlstd{shareoftotalN} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0.049487}\hlstd{,} \hlnum{0.1520981}\hlstd{,} \hlnum{0.626616}\hlstd{,} \hlnum{0.171799}\hlstd{)}
\hlstd{overall_ate} \hlkwb{<-}\hlkwd{sum}\hlstd{(ests}\hlopt{*}\hlstd{shareoftotalN)}
\hlstd{overall_ate}
\end{alltt}
\begin{verbatim}
## [1] -0.007827
\end{verbatim}
\end{kframe}
\end{knitrout}

\item When analyzing block randomized experiments, researchers frequently use regression to estimate the ATE by regressing the outcome on the treatment and indicator variables for each of the blocks (omitting one block if the regression includes an intercept.)  This regression estimator places extra weight on blocks that allocate approximately half of the subjects to the treatment condition (i.e., $P_j  = 0.5$) because these blocks tend to estimate the within-block ATE with less sampling variability. Compare the four OLS weights to the weights $W_j$ used in part (c).\\
Answer:\\
The weights used in part (c) are based on the share of the subject pool that is in each block.  This weighting scheme places a great deal of weight on the relatively large Michigan block. By contrast, the OLS weights are a blend of the number of subjects in each block and each block's balance between treatment and control allocations. Because the blocks do not differ very much in terms of their allocation rates, the OLS weights tend to be similar across blocks.
\item Regression provides an easy way to calculate the weighted estimate of the ATE in part (c) above. For each treatment subject $i$, compute the proportion of subjects in the same block who were assigned to the treatment group.  For control subjects, compute the proportion of subjects in the same block who were assigned to the control group.  Call this variable $q_i$.  Regress outcomes on treatment, weighting each observation by $1/q_i$, and show that this type of weighted regression produces the same estimate as weighting the estimated ATEs for each block.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Y} \hlkwb{<-} \hlstd{phones}\hlopt{$}\hlstd{vote02}
\hlstd{block} \hlkwb{<-} \hlstd{phones}\hlopt{$}\hlstd{strata}
\hlstd{Z} \hlkwb{<-} \hlstd{phones}\hlopt{$}\hlstd{treat2}

\hlcom{## Proportion of subjects in each block assigned to treatment}
\hlstd{block.pr} \hlkwb{<-} \hlkwd{tapply}\hlstd{(Z, block, mean)}

\hlstd{q} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{length}\hlstd{(Y))}

\hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlnum{4}\hlstd{)\{}
  \hlstd{q[block}\hlopt{==}\hlstd{i]} \hlkwb{<-} \hlstd{block.pr[i]}\hlopt{*}\hlstd{Z[block}\hlopt{==}\hlstd{i]} \hlopt{+} \hlstd{(}\hlnum{1}\hlopt{-}\hlstd{block.pr[i])}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{Z[block}\hlopt{==}\hlstd{i])}
\hlstd{\}}

\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{Z,} \hlkwc{weights}\hlstd{=}\hlnum{1}\hlopt{/}\hlstd{q)}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ Z, weights = 1/q)
## 
## Weighted Residuals:
##    Min     1Q Median     3Q    Max 
## -4.051 -0.469 -0.469  0.537  4.786 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  0.466198   0.000727  641.31  < 2e-16 ***
## Z           -0.007828   0.001028   -7.61  2.7e-14 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.705 on 940713 degrees of freedom
## Multiple R-squared:  6.16e-05,	Adjusted R-squared:  6.06e-05 
## F-statistic:   58 on 1 and 940713 DF,  p-value: 2.65e-14
\end{verbatim}
\end{kframe}
\end{knitrout}

The coefficient on the treatment indicator is \ensuremath{-0.0078}, which is the same as was found in part c.

\end{enumerate}


\section*{Question 10}


The 2003 Kansas City voter mobilization experiment described in Chapter 3 is a cluster randomized design in which 28 precincts comprising 9,712 voters were randomly assigned to treatment and control.\footnote{Arceneaux 2005.} The study contains a wealth of covariates: the registrar recorded whether each voter participated in elections dating back to 1996.  The dataset may be obtained at isps.research.yale.edu/FEDAI.

\begin{enumerate}[a)]

\item Test the balance of the treatment and control groups by looking at whether past turnout predicts treatment assignment.  Regress treatment assignment on the entire set of past votes, and calculate the F-statistic.  Use randomization inference to test the null hypothesis that none of the past turnout variables predict treatment assignment. Remember that to simulate the distribution of the F-statistic, you must generate 1,000 random cluster assignments and calculate the F-statistic for each simulated assignment.  Judging from the p-value of this test, what does the F-statistic seem to suggest about whether subjects in the treatment and control groups have comparable background characteristics?  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Z} \hlkwb{<-}  \hlstd{kansas}\hlopt{$}\hlstd{treatmen}
\hlstd{Y} \hlkwb{<-} \hlstd{kansas}\hlopt{$}\hlstd{vote03}
\hlstd{clust} \hlkwb{<-} \hlstd{kansas}\hlopt{$}\hlstd{unit}
\hlstd{covs} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(kansas[,}\hlnum{2}\hlopt{:}\hlnum{21}\hlstd{])}  \hlcom{# covariates are past voter turnout}

\hlstd{probs} \hlkwb{<-} \hlkwd{genprobexact}\hlstd{(Z,}\hlkwc{clustvar}\hlstd{=clust)}  \hlcom{# subjects are clustered by precinct}
\hlstd{perms} \hlkwb{<-} \hlkwd{genperms}\hlstd{(Z,}\hlkwc{maxiter}\hlstd{=}\hlnum{1000}\hlstd{,}\hlkwc{clustvar}\hlstd{=clust)}    \hlcom{# clustered assignment}
\end{alltt}
\begin{verbatim}
## Too many permutations to use exact method.
## Defaulting to approximate method.
## Increase maxiter to at least 40116600 to perform exact estimation.
\end{verbatim}
\begin{alltt}
\hlstd{numiter} \hlkwb{<-} \hlkwd{ncol}\hlstd{(perms)}

\hlstd{Fstat} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{lm}\hlstd{(Z}\hlopt{~}\hlstd{covs))}\hlopt{$}\hlstd{fstatistic[}\hlnum{1}\hlstd{]}   \hlcom{# F-statistic from actual data}

\hlstd{Fstatstore} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,numiter)}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{numiter) \{}
  \hlstd{Fstatstore[i]} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{lm}\hlstd{(perms[,i]}\hlopt{~}\hlstd{covs))}\hlopt{$}\hlstd{fstatistic[}\hlnum{1}\hlstd{]}
        \hlstd{\}}

\hlstd{p.value} \hlkwb{<-} \hlkwd{mean}\hlstd{(Fstatstore} \hlopt{>=} \hlstd{Fstat)}
\hlstd{p.value}
\end{alltt}
\begin{verbatim}
## [1] 0.936
\end{verbatim}
\end{kframe}
\end{knitrout}

Using randomization inference, we recover a $p$-value of 0.936; we therefore cannot reject the null hypothesis of random assignment.

\item Regress turnout in 2003 (after the treatment was administered) on the experimental assignment and the full set of covariates. Interpret the estimated ATE. Use randomization inference to test the sharp null hypothesis that experimental assignment had no effect on any subject's decision to vote.  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ate} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,Z,}\hlkwc{X}\hlstd{=covs,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y,Z,}\hlkwc{ate}\hlstd{=}\hlnum{0}\hlstd{)}
\hlstd{distout} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{X}\hlstd{=covs,}\hlkwc{prob}\hlstd{=probs)}
\hlstd{p.value.onetailed} \hlkwb{<-} \hlkwd{mean}\hlstd{(distout} \hlopt{>=} \hlstd{ate)}

\hlstd{ate}
\end{alltt}
\begin{verbatim}
##       Z 
## 0.05596
\end{verbatim}
\begin{alltt}
\hlstd{p.value.onetailed}
\end{alltt}
\begin{verbatim}
## [1] 0.005
\end{verbatim}
\end{kframe}
\end{knitrout}

The estimate of the treatment effect is 0.056, implying that treatment increased turnout by 5.6 percentage points. This finding is statistically significant. Under the sharp null, estimates as large or larger only occur 0.5\% of the time.

\item When analyzing cluster randomized experiments with clusters of varying size, one concern is that difference-in-means estimation is prone to bias. This concern also applies to regression.  In order to sidestep this problem, researchers may choose to use the difference-in-totals estimator in equation (3.24) to estimate the ATE. Estimate the ATE using this estimator.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ateHT} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y,Z,}\hlkwc{prob}\hlstd{=probs,}\hlkwc{HT}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{ateHT}
\end{alltt}
\begin{verbatim}
## [1] 0.05395
\end{verbatim}
\end{kframe}
\end{knitrout}

The difference-in-totals estimate of the treatment effect is that treatment increased turnout by 5.4 percentage points.

\item Use randomization inference to test the sharp null hypothesis that treatment assignment had no effect, using the difference-in-totals estimator.  

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{distoutHT} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{prob}\hlstd{=probs,}\hlkwc{HT}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{p.value.onesidedHT} \hlkwb{<-} \hlkwd{mean}\hlstd{(distoutHT} \hlopt{>=} \hlstd{ateHT)}
\hlstd{p.value.onesidedHT}
\end{alltt}
\begin{verbatim}
## [1] 0.198
\end{verbatim}
\end{kframe}
\end{knitrout}

Estimates generated under the sharp null equaled or exceeded the observed difference-in-totals 19.8\% of the time, meaning we cannot reject the null.

\item The difference-in-totals estimator can generate imprecise estimates, but its precision can be improved by incorporating information about covariates. Create a new outcome variable that is the difference between a subject's turnout (1 = vote, 0 = abstain) and the average rate of turnout in all past elections. Now, using this ``differenced'' outcome variable, estimate the ATE using the difference-in-totals estimator, and test the sharp null hypothesis of no effect.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Y_diff} \hlkwb{<-} \hlstd{Y} \hlopt{-} \hlkwd{rowMeans}\hlstd{(covs)}

\hlstd{ateHT2} \hlkwb{<-} \hlkwd{estate}\hlstd{(Y_diff,Z,}\hlkwc{prob}\hlstd{=probs,}\hlkwc{HT}\hlstd{=}\hlnum{TRUE}\hlstd{)}  \hlcom{# difference-in-differenced totals}
\hlstd{Ys} \hlkwb{<-} \hlkwd{genouts}\hlstd{(Y_diff,Z,}\hlkwc{ate}\hlstd{=}\hlnum{0}\hlstd{)}
\hlstd{distoutHT2} \hlkwb{<-} \hlkwd{gendist}\hlstd{(Ys,perms,}\hlkwc{prob}\hlstd{=probs,}\hlkwc{HT}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{p.value.onesidedHT2} \hlkwb{<-} \hlkwd{mean}\hlstd{(distoutHT2} \hlopt{>=} \hlstd{ateHT2)}

\hlstd{ateHT2}
\end{alltt}
\begin{verbatim}
## [1] 0.04874
\end{verbatim}
\begin{alltt}
\hlstd{p.value.onesidedHT2}
\end{alltt}
\begin{verbatim}
## [1] 0.012
\end{verbatim}
\end{kframe}
\end{knitrout}

Using the differenced outcome variable tightened our estimates -- the $p$-value under the sharp null is now 0.012, meaning we can reject the sharp null of no effect for any unit.

\end{enumerate}











\end{document}

