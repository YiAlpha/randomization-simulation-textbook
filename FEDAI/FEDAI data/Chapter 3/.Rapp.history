table(tip$treatment)
look at outcome distribution#
hist(tip$tip,breaks=25,col="gray")
aggregate(tip ~ treatment,data=tip,mean)#
aggregate(tip ~ treatment+female_dummy,data=tip,mean)
mean(tip$tip[treatment==0])
mean(tip$tip[tip$treatment==0])
bivariate <- lm(tip ~ treatment,data=tip)#
summary(bivariate)
plot(tip$treatment,tip$tip)#
abline(a=bivariate$coef[1],b=bivariate$coef[2])
interact <- lm(tip ~ treatment+female_dummy+treatment:female_dummy,data=tip)#
summary(interact)
set.seed(1234)#
sims <- 10000  # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope#
c <- 0         # stipulate another true slope#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	lmfit1 <- lm(Y ~ X)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X + Z)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error
set.seed(1234)#
sims <- 10000  # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope#
c <- -2         # stipulate another true slope#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	lmfit1 <- lm(Y ~ X)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X + Z)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
set.seed(1234)#
sims <- 10000  # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope for X#
c <- -2        # stipulate another true slope for Z#
e1 <- 0        # stipulate measurement error variance in X#
e2 <- 0        # stipulate measurement error variance in Z#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	X_star <- X + sqrt(e1)*rnorm(n,0,1)#
	Z_star <- Z + sqrt(e2)*rnorm(n,0,1)#
	lmfit1 <- lm(Y ~ X_star)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X_star + Z_star)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
set.seed(1234)#
sims <- 10000  # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope for X#
c <- -2        # stipulate another true slope for Z#
e1 <- 3        # stipulate measurement error variance in X#
e2 <- 1        # stipulate measurement error variance in Z#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	X_star <- X + sqrt(e1)*rnorm(n,0,1)#
	Z_star <- Z + sqrt(e2)*rnorm(n,0,1)#
	lmfit1 <- lm(Y ~ X_star)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X_star + Z_star)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
var(X)
set.seed(1234)#
sims <- 1000   # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope for X#
c <- -2        # stipulate another true slope for Z#
e1 <- 1        # stipulate measurement error variance in X#
e2 <- 1        # stipulate measurement error variance in Z#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	X_star <- X + sqrt(e1)*rnorm(n,0,1)#
	Z_star <- Z + sqrt(e2)*rnorm(n,0,1)#
	lmfit1 <- lm(Y ~ X_star)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X_star + Z_star)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
set.seed(1234)#
sims <- 1000   # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope for X#
c <- 0        # stipulate another true slope for Z#
e1 <- .1        # stipulate measurement error variance in X#
e2 <- .1        # stipulate measurement error variance in Z#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	X_star <- X + sqrt(e1)*rnorm(n,0,1)#
	Z_star <- Z + sqrt(e2)*rnorm(n,0,1)#
	lmfit1 <- lm(Y ~ X_star)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X_star + Z_star)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
set.seed(1234)#
sims <- 1000   # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope for X#
c <- 0        # stipulate another true slope for Z#
e1 <- .01        # stipulate measurement error variance in X#
e2 <- .01        # stipulate measurement error variance in Z#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	X_star <- X + sqrt(e1)*rnorm(n,0,1)#
	Z_star <- Z + sqrt(e2)*rnorm(n,0,1)#
	lmfit1 <- lm(Y ~ X_star)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X_star + Z_star)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
set.seed(1234)#
sims <- 1000   # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope for X#
c <- 0        # stipulate another true slope for Z#
e1 <- .001        # stipulate measurement error variance in X#
e2 <- .001        # stipulate measurement error variance in Z#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	X_star <- X + sqrt(e1)*rnorm(n,0,1)#
	Z_star <- Z + sqrt(e2)*rnorm(n,0,1)#
	lmfit1 <- lm(Y ~ X_star)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X_star + Z_star)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
set.seed(1234)#
sims <- 1000   # number of hypothetical studies#
n <- 10        # number of observations in each study#
#
a <- 3         # stipulate an true intercept#
b <- 5         # stipulate a true slope for X#
c <- 0        # stipulate another true slope for Z#
e1 <- .000        # stipulate measurement error variance in X#
e2 <- .000        # stipulate measurement error variance in Z#
#
# "initialize" the variables "out" and "Y" as NA codes#
out_bivar   <- rep(NA,sims)  #
out_multvar <- rep(NA,sims)  #
#
Y <- rep(NA,n)#
X <- runif(n) # generate the independent variable#
Z <- 1*X + 1* runif(n) # generate a covariate (you can vary the covariance)#
#
for (i in 1:sims) {#
	Y <- a + b*X + c*Z + rnorm(n,0,1) # the "true" regression model#
	X_star <- X + sqrt(e1)*rnorm(n,0,1)#
	Z_star <- Z + sqrt(e2)*rnorm(n,0,1)#
	lmfit1 <- lm(Y ~ X_star)#
	out_bivar[i] <- lmfit1$coeff[2]	# store the estimated b	#
	lmfit2 <- lm(Y ~ X_star + Z_star)#
	out_multvar[i] <- lmfit2$coeff[2]	# store the estimated b	#
}#
#
mean(out_bivar)  # Is the regression estimator of the slope unbiased?#
sd(out_bivar)    # the empirical standard error: approximately s/sqrt(n*var(x))#
#
mean(out_multvar)  # Is the regression estimator of the slope unbiased?#
sd(out_multvar)    # the empirical standard error: approximately s/sqrt(n*var(x)(1-R2_{xz}))
rm(list=ls())#
library(foreign)#
#
ps <- read.dta("/Users/donaldgreen/Dropbox/Teaching 2012-2013/Stats & Data Analysis/Exams/CC PS majors.dta")#
#
names(ps)
attach(ps)#
summary(c(year,PS_majors))
summary(PS_majors)
regression and plot#
bivariate <- lm(PS_majors ~ year)#
summary(bivariate)
plot(year,PS_majors)#
abline(a=bivariate$coef[1],b=bivariate$coef[2])
ps
regression and plot#
bivariate <- lm(PS_majors ~ year)#
summary(bivariate)#
#
plot(year,PS_majors)#
abline(a=bivariate$coef[1],b=bivariate$coef[2])
rm(list=ls())#
library(foreign)#
#
tip <- read.dta("/Users/donaldgreen/Dropbox/Teaching 2012-2013/Stats & Data Analysis/Datasets/Tipping/Rind and Bordia Tipping Experiment.dta")#
#
names(tip)#
#
# look at outcome distribution#
hist(tip$tip,breaks=25,col="gray")#
#
# look at means#
aggregate(tip ~ treatment,data=tip,mean)#
aggregate(tip ~ treatment+female_dummy,data=tip,mean)#
#
# regression and plot#
bivariate <- lm(tip ~ treatment,data=tip)#
summary(bivariate)#
#
plot(tip$treatment,tip$tip)#
abline(a=bivariate$coef[1],b=bivariate$coef[2])
interactive regression and plot#
interact <- lm(tip ~ treatment+female_dummy+treatment:female_dummy,data=tip)#
summary(interact)#
plot(tip$treatment,tip$tip,xlim=c(0,1),ylim=c(0,100),pch=16,main="Tipping by Treatment and Gender of Server")#
points(tip$treatment[tip$female_dummy==0],tip$tip[tip$female_dummy==0],col="red",pch=1)#
points(tip$treatment[tip$female_dummy==1],tip$tip[tip$female_dummy==1],col="black",pch=21)#
#
interact.0 <- lm(tip ~ treatment,data=tip,subset=female_dummy==0)#
interact.1 <- lm(tip ~ treatment,data=tip,subset=female_dummy==1)#
abline(a=interact.0$coef[1],b=interact.0$coef[2],col="red",lty=1)#
abline(a=interact.1$coef[1],b=interact.1$coef[2],col="black",lty=2)#
legend(.05,70,legend=c("male","female"),fill=c("red","black"))
load("/Users/donaldgreen/Dropbox/Double sampling/analysis/SimGraph1.RData")
calculate population mean, under ignorability in second round#
ds.mean <- function(p1,p2,y1m,y2m) p1*y1m + (1-p1)*y2m#
#
# calculate sampling variance#
ds.var <- function(n1,n2,p1,p2,s1,s2,y1m,y2m) p1*n1/n1^2 * s1^2 + (1-p1*n1)^2/(n2*n1^2)*s2^2 + (1-p1*n1)*p1/n1^3*(y2m-y1m)^2#
#
# calculate manski bounds#
ds.manski <- function(p1,p2,y1m,y2m.nm,minY,maxY) {#
	c1 <- p1*y1m + (1-p1)*p2*y2m#
	c2 <- (1-p1)*(1-p2)#
	return(c(c1+c2*minY,c1+c2*maxY))#
}#
#
# calcuate manski bound CIs#
ds.manski.cis <- function(n1,n2,p1,p2,s1,s2.nm,y1m,y2m.nm,minY,maxY,alpha) {#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
	lower.se <- ds.var(n1,n2,p1,p2,s1,gen.var(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY),y1m,gen.mean(y2m.nm,p2,lower.bound=TRUE,minY,maxY))^.5 #
	upper.se <- ds.var(n1,n2,p1,p2,s1,gen.var(y2m.nm,s2.nm,p2,lower.bound=FALSE,minY,maxY),y1m,gen.mean(y2m.nm,p2,lower.bound=FALSE,minY,maxY))^.5 #
#
	sig <- qnorm(1-(1-alpha)/2)#
#
	return(ds.manski(p1,p2,y1m,y2m.nm,minY,maxY)+c(-sig*lower.se,sig*upper.se))#
}#
# find the n1,n2 (number of attempts to measure) that minimize sampling variance#
optim.var <- function(p1,p2,s1,s2,y1m,y2m,c1a,c1r,c2a,c2r,budget) {#
	# cheap starting values#
	n1g = budget/(c1a+c1r*p1) * .5#
	n2g = 4#
	# constraints#
    #- (c1a+c1r*p1)*n1 - (c2a+c2r*p2)*n2 + budget >= 0#
    # n1 > 2   #
    # n2 > 2 #
    # (1-p1)*n1 - n2 > 2#
	constrMat <- rbind(c(- (c1a+c1r*p1),-(c2a+c2r*p2)),c(1,0),c((1-p1),-1),c(0,1))#
	constrVec <- c(-budget,2,2,2)#
	# ds.var with variables rearranged for optimization#
	ds.passthrough <- function(ns,p1,p2,s1,s2,y1m,y2m) ds.var(ns[1],ns[2],p1,p2,s1,s2,y1m,y2m)#
	optimD <- constrOptim(c(n1g,n2g),ds.passthrough,grad=NULL,ui=constrMat,ci=constrVec,p1=p1,p2=p2,s1=s1,s2=s2,y1m=y1m,y2m=y2m)#
	return(c(n1 = optimD$par[1],n2 = optimD$par[2],var=optimD$value))	#
}#
#
# calculate two-sample sampling variance#
ds.var.2s <- function(treatment.vec,control.vec) ds.var(treatment.vec[1],treatment.vec[2],treatment.vec[3],treatment.vec[4],treatment.vec[5],treatment.vec[6],treatment.vec[7],treatment.vec[8]) + ds.var(control.vec[1],control.vec[2],control.vec[3],control.vec[4],control.vec[5],control.vec[6],control.vec[7],control.vec[8])#
#
# find the n1.t,n2.t,n1.c,n2.c that minimize sampling variance#
optim.var.2s <- function(p1.t,p2.t,s1.t,s2.t,y1m.t,y2m.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.c,y1m.c,y2m.c,c1a.c,c1r.c,c2a.c,c2r.c,budget) {#
	# cheap starting values, need to improve#
	n1g.t = 3/p1.t#
	n2g.t = 3#
	n1g.c = 3/p1.t#
	n2g.c = 3#
	# constraints#
    # - (c1a.t+c1r.t*p1.t)*n1 - (c2a.t+c2r.t*p2,t)*n2 + budget >= 0#
    # n1.t >= 2   #
    # n2.t >= 2 #
    # (1-p1.t)*n1.t - n2.t >= 2#
	constrMat <- rbind(#
	     c(-(c1a.t+c1r.t*p1.t),-(c2a.t+c2r.t*p2.t),-(c1a.c+c1r.c*p1.c),-(c2a.c+c2r.c*p2.c)),#
	     c(1,0,0,0),#
	     c(0,1,0,0),#
	     c((1-p1.t),-1,0,0),#
	     c(0,0,1,0),#
	     c(0,0,0,1),#
	     c(0,0,(1-p1.c),-1))#
	constrVec <- c(#
		-budget,#
		2,#
		2,#
		2,#
		2,#
		2,#
		2)#
	# ds.var with variables rearranged for optimization#
	ds.passthrough <- function(ns,p1.t,p2.t,s1.t,s2.t,y1m.t,y2m.t,p1.c,p2.c,s1.c,s2.c,y1m.c,y2m.c) ds.var.2s( #
	c(ns[1],ns[2],p1.t,p2.t,s1.t,s2.t,y1m.t,y2m.t),#
	c(ns[3],ns[4],p1.c,p2.c,s1.c,s2.c,y1m.c,y2m.c))#
	optimD <- constrOptim(c(n1g.t,n2g.t,n1g.c,n2g.c),ds.passthrough,grad=NULL,ui=constrMat,ci=constrVec,p1.t=p1.t,p2.t=p2.t,s1.t=s1.t,s2.t=s2.t,y1m.t=y1m.t,y2m.t=y2m.t,p1.c=p1.c,p2.c=p2.c,s1.c=s1.c,s2.c=s2.c,y1m.c=y1m.c,y2m.c=y2m.c)#
	return(c(optimD$par,var=optimD$value))#
}#
# find the n1,n2 (number of attempts to measure) that minimize width of Manski CIs#
optim.manski <- function(p1,p2,s1,s2.nm,y1m,y2m.nm,c1a,c1r,c2a,c2r,budget,minY,maxY) {#
	# cheap starting values#
	n1g = 4/p1#
	n2g = 4#
	# constraints#
    #- (c1a+c1r*p1)*n1 - (c2a+c2r*p2)*n2 + budget >= 0#
    # n1 > 2   #
    # n2 > 2 #
    # (1-p1)*n1 - n2 > 2#
	constrMat <- rbind(c(- (c1a+c1r*p1),-(c2a+c2r*p2)),c(1,0),c((1-p1),-1),c(0,1))#
	constrVec <- c(-budget,2,2,2)#
	# compute width of 95% CIs#
	ds.passthrough <- function(ns,p1,p2,s1.nm,s2.nm,y1m,y2m.nm,minY,maxY) {#
	diff(ds.manski.cis(ns[1],ns[2],p1,p2,s1,s2.nm,y1m,y2m.nm,minY,maxY,0.95))#
	# ds.var(ns[1],ns[2],p1,p2,s1,gen.var(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY),y1m,gen.mean(y2m.nm,p2,lower.bound=TRUE,minY,maxY))^.5 + ds.var(ns[1],ns[2],p1,p2,s1,gen.var(y2m.nm,s2.nm,p2,lower.bound=FALSE,minY,maxY),y1m,gen.mean(y2m.nm,p2,lower.bound=FALSE,minY,maxY))^.5#
	}#
	optimD <- constrOptim(c(n1g,n2g),ds.passthrough,grad=NULL,ui=constrMat,ci=constrVec,p1=p1,p2=p2,s1=s1,s2.nm=s2.nm,y1m=y1m,y2m.nm=y2m.nm,minY=minY,maxY=maxY)#
	return(c(n1 = optimD$par[1],n2 = optimD$par[2],ci.addedwidth=optimD$value))	#
}#
#
# Sample code -- 1s (standard) variance#
#
p1 <- .10 # expected proportion nonmissing in first round#
p2 <- .50 # expected proportion nonmissing in second round#
s1 <- 10 # standard deviation in the first round#
s2 <- 5 # standard deviation in the second round#
y1m <- 10 # mean in the first round#
y2m <- 5 # mean in the second round#
budget <- 100 # overall budget#
c1a <- .05 # cost of attempting to measure in first round#
c1r <- .10 # cost of measuring in first round (above attempting)#
c2a <- .10 # cost of attempting to measure in second round#
c2r <- 1 # cost of measuring in second round (above attempting)#
#
find.opt <- optim.var(p1,p2,s1,s2,y1m,y2m,c1a,c1r,c2a,c2r,budget)#
find.opt#
#
# Sample code -- 1s (standard) Manski#
#
p1 <- .10 # expected proportion nonmissing in first round#
p2 <- .50 # expected proportion nonmissing in second round#
s1 <- 10 # standard deviation in the first round#
s2.nm <- 5 # standard deviation in the second round, among sampled#
y1m <- 10 # mean in the first round#
y2m.nm <- 10 # mean in the second round, among sampled#
budget <- 100 # overall budget#
c1a <- .05 # cost of attempting to measure in first round#
c1r <- .10 # cost of measuring in first round (above attempting)#
c2a <- .10 # cost of attempting to measure in second round#
c2r <- 1 # cost of measuring in second round (above attempting)#
maxY <- 10#
minY <- 0#
#
# compute manski bounds and CIs#
ds.manski(p1,p2,y1m,y2m.nm,minY,maxY)#
ds.manski.cis(n1=100,n2=10,p1,p2,s1,s2.nm,y1m,y2m.nm,minY,maxY,0.95)#
ds.manski.cis(n1=1000000,n2=1000000,p1,p2,s1,s2.nm,y1m,y2m.nm,minY,maxY,0.95)#
#
find.opt.manski <- optim.manski(p1,p2,s1,s2.nm,y1m,y2m.nm,c1a,c1r,c2a,c2r,budget,minY,maxY)#
find.opt.manski#
#
# Sample code -- 2s#
#
p1.t <- .10 # expected proportion nonmissing in first round#
p2.t <- .50 # expected proportion nonmissing in second round#
s1.t <- 10 # standard deviation in the first round#
s2.t <- 5 # standard deviation in the second round#
y1m.t <- 10 # mean in the first round#
y2m.t <- 5 # mean in the second round#
c1a.t <- .05 # cost of attempting to measure in first round#
c1r.t <- .10 # cost of measuring in first round (above attempting)#
c2a.t <- .10 # cost of attempting to measure in second round#
c2r.t <- 1 # cost of measuring in second round (above attempting)#
p1.c <- .10 # expected proportion nonmissing in first round#
p2.c <- .50 # expected proportion nonmissing in second round#
s1.c <- 10 # standard deviation in the first round#
s2.c <- 5 # standard deviation in the second round#
y1m.c <- 10 # mean in the first round#
y2m.c <- 5 # mean in the second round#
budget <- 100 # overall budget#
c1a.c <- .05 # cost of attempting to measure in first round#
c1r.c <- .10 # cost of measuring in first round (above attempting)#
c2a.c <- .10 # cost of attempting to measure in second round#
c2r.c <- 1 # cost of measuring in second round (above attempting)#
#
find.opt.2s <- optim.var.2s(p1,p2,s1,s2,y1m,y2m,c1a,c1r,c2a,c2r,p1,p2,s1,s2,y1m,y2m,c1a,c1r,c2a,c2r,budget)#
#
find.opt.2s
?ri
??ri
?ri
library(ri)
?ri
rm(list = ls())#
#
# calculate population mean, under ignorability in second round#
ds.mean <- function(p1,p2,y1m,y2m) p1*y1m + (1-p1)*y2m#
#
# calculate sampling variance#
ds.var <- function(n1,n2,p1,p2,s1,s2,y1m,y2m) p1*n1/n1^2 * s1^2 + (1-p1*n1)^2/(n2*n1^2)*s2^2 + (1-p1*n1)*p1/n1^3*(y2m-y1m)^2#
#
# calculate manski bounds#
ds.manski <- function(p1,p2,y1m,y2m.nm,minY,maxY) {#
	const1 <- p1*y1m + (1-p1)*p2*y2m#
	const2 <- (1-p1)*(1-p2)#
	return(c(const1+const2*minY,const1+const2*maxY))#
}#
#
# calculate two-sample sampling variance#
ds.var.2s <- function(treatment.vec,control.vec) ds.var(treatment.vec[1],treatment.vec[2],treatment.vec[3],treatment.vec[4],treatment.vec[5],treatment.vec[6],treatment.vec[7],treatment.vec[8]) + ds.var(control.vec[1],control.vec[2],control.vec[3],control.vec[4],control.vec[5],control.vec[6],control.vec[7],control.vec[8])#
#
# estimate manski bounds, two-sample:#
ds.manski.2s <- function(p1.t,p2.t,y1m.t,y2m.t,p1.c,p2.c,y1m.c,y2m.c,minY,maxY) {#
	const1.t <- p1.t*y1m.t + (1-p1.t)*p2.t*y2m.t#
	const2.t <- (1-p1.t)*(1-p2.t)#
	const1.c <- p1.c*y1m.c + (1-p1.c)*p2.c*y2m.c#
	const2.c <- (1-p1.c)*(1-p2.c)#
	return(c(const1.t+const2.t*minY - (const1.c+const2.c*maxY),const1.t+const2.t*maxY - (const1.c+const2.c*minY)))#
	}#
ds.manski.cis.2s <- function(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha) {#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
#
	y2m.t.L <- gen.mean(y2m.nm.t,p2.t,lower.bound=TRUE,minY,maxY)#
	y2m.t.U <- gen.mean(y2m.nm.t,p2.t,lower.bound=FALSE,minY,maxY)#
	y2m.c.L <- gen.mean(y2m.nm.c,p2.c,lower.bound=TRUE,minY,maxY)#
	y2m.c.U <- gen.mean(y2m.nm.c,p2.c,lower.bound=FALSE,minY,maxY)#
	s2.t.L <- gen.var(y2m.nm.t,s2.nm.t,p2.t,lower.bound=TRUE,minY,maxY)^.5#
	s2.t.U <- gen.var(y2m.nm.t,s2.nm.t,p2.t,lower.bound=FALSE,minY,maxY)^.5#
	s2.c.L <- gen.var(y2m.nm.c,s2.nm.t,p2.c,lower.bound=TRUE,minY,maxY)^.5#
	s2.c.U <- gen.var(y2m.nm.c,s2.nm.t,p2.c,lower.bound=FALSE,minY,maxY)^.5#
#
	manski.bounds.est <- ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
	lower.bound.est <- ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)[1]#
	upper.bound.est <- ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)[2]#
	lower.bound.var.est <- ds.var.2s(c(n1.t,n2.t,p1.t,p2.t,s1.t,s2.t.L,y1m.t,y2m.t.L),c(n1.c,n2.c,p1.c,p2.c,s1.c,s2.c.U,y1m.c,y2m.c.U))#
	upper.bound.var.est <- ds.var.2s(c(n1.t,n2.t,p1.t,p2.t,s1.t,s2.t.U,y1m.t,y2m.t.U),c(n1.c,n2.c,p1.c,p2.c,s1.c,s2.c.L,y1m.c,y2m.c.L))#
	 sig <- qnorm(1-(1-alpha)/2)#
	 return(c(lower.bound.est - sig*lower.bound.var.est^.5,upper.bound.est + sig*upper.bound.var.est^.5))#
}#
#
# find the n1,n2 (number of attempts to measure) that minimize width of Manski CIs#
optim.manski.2s <- function(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,starting.values = NULL,mu=1e-04) {#
	if( is.null(starting.values)) {#
	# cheap starting values#
	n1g.t = 5/p1.t#
	n2g.t = 4#
	n1g.c = 5/p1.c#
	n2g.c = 4} else {#
		n1g.t = starting.values[1]#
		n2g.t = starting.values[2]#
		n1g.c = starting.values[3]#
		n2g.c = starting.values[4]#
	}#
	# constraints#
    #- (c1a.t+c1r.t*p1.t)*n1.t - (c2a.t+c2r.t*p2.t)*n2.t - (c1a.c+c1r.c*p1.c)*n1.c - (c2a.c+c2r.c*p2.c)*n2.c + budget >= 0#
    # n1.t > 2   #
    # n2.t > 2 #
    # n1.c > 2   #
    # n2.c > 2 #
    # (1-p1.t)*n1.t - n2.t > 2#
    # (1-p1.c)*n1.c - n2.c > 2#
	# constraints#
    # - (c1a.t+c1r.t*p1.t)*n1 - (c2a.t+c2r.t*p2,t)*n2 + budget >= 0#
    # n1.t >= 2   #
    # n2.t >= 2 #
    # (1-p1.t)*n1.t - n2.t >= 2#
    # same as above, verify by c+p#
	constrMat <- rbind(c(-(c1a.t+c1r.t*p1.t),-(c2a.t+c2r.t*p2.t),-(c1a.c+c1r.c*p1.c),-(c2a.c+c2r.c*p2.c)),#
							c(1,0,0,0),#
							c(0,1,0,0),#
							c(0,0,1,0),#
							c(0,0,0,1),#
							c((1-p1.t),-1,0,0),#
							c(0,0,(1-p1.c),-1)#
							)#
	constrVec <- c(-budget,2,2,2,2,2,2)#
	# compute width of 95% CIs#
	ds.passthrough.2s <- function(ns,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha) diff(ds.manski.cis.2s(ns[1],ns[2],ns[3],ns[4],p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha))#
	# grad.passthrough.2s <- grad(ds.passthrough.2s,ns,method.args=list(ns,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha))#
	optimD <- constrOptim(c(n1g.t,n2g.t,n1g.c,n2g.c),ds.passthrough.2s,grad=NULL,ui=constrMat,ci=constrVec,p1.t=p1.t,p2.t=p2.t,s1.t=s1.t,s2.nm.t=s2.nm.t,y1m.t=y1m.t,y2m.nm.t=y2m.nm.t,c1a.t=c1a.t,c1r.t=c1r.t,c2a.t=c2a.t,c2r.t=c2r.t,p1.c=p1.c,p2.c=p2.c,s1.c=s1.c,s2.nm.c=s2.nm.c,y1m.c=y1m.c,y2m.nm.c=y2m.nm.c,minY=minY,maxY=maxY,alpha=alpha,outer.iterations=10000,outer.eps = .Machine$double.eps^.8,mu=mu)#
	return(c(n1.t = optimD$par[1],n2.t = optimD$par[2],n1.c = optimD$par[3],n2.c = optimD$par[4],ci.width=optimD$value))	#
}#
#
############
#
# Sample code -- 2s#
#
p1.t <- .10 # expected proportion nonmissing in first round#
p2.t <- .50 # expected proportion nonmissing in second round#
s1.t <- 2 # standard deviation in the first round#
s2.nm.t <- 5 # standard deviation in the second round#
y1m.t <- 5 # mean in the first round#
y2m.nm.t <- 5 # mean in the second round#
c1a.t <- .05 # cost of attempting to measure in first round#
c1r.t <- .10 # cost of measuring in first round (above attempting)#
c2a.t <- .10 # cost of attempting to measure in second round#
c2r.t <- 1 # cost of measuring in second round (above attempting)#
#
p1.c <- .10 # expected proportion nonmissing in first round#
p2.c <- .50 # expected proportion nonmissing in second round#
s1.c <- 2 # standard deviation in the first round#
s2.nm.c <- 5 # standard deviation in the second round#
y1m.c <- 5 # mean in the first round#
y2m.nm.c <- 5 # mean in the second round#
c1a.c <- .05 # cost of attempting to measure in first round#
c1r.c <- .10 # cost of measuring in first round (above attempting)#
c2a.c <- .10 # cost of attempting to measure in second round#
c2r.c <- 1 # cost of measuring in second round (above attempting)#
#
maxY <- 10#
minY <- 0#
alpha <- 0.95#
budget <- 100 # overall budget#
#
ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
#
find.opt.manski.2s <- optim.manski.2s(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,starting.values=NULL,mu=1e-05)#
#
find.opt.manski.2s#
#
n1.t <- find.opt.manski.2s[1]#
n2.t <- find.opt.manski.2s[2]#
n1.c <- find.opt.manski.2s[3]#
n2.c <- find.opt.manski.2s[4]#
#
ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)
rm(list = ls())#
#
# calculate population mean, under ignorability in second round#
ds.mean <- function(p1,p2,y1m,y2m) p1*y1m + (1-p1)*y2m#
#
# calculate sampling variance#
ds.var <- function(n1,n2,p1,p2,s1,s2,y1m,y2m) p1*n1/n1^2 * s1^2 + (1-p1*n1)^2/(n2*n1^2)*s2^2 + (1-p1*n1)*p1/n1^3*(y2m-y1m)^2#
#
# calculate manski bounds#
ds.manski <- function(p1,p2,y1m,y2m.nm,minY,maxY) {#
	const1 <- p1*y1m + (1-p1)*p2*y2m#
	const2 <- (1-p1)*(1-p2)#
	return(c(const1+const2*minY,const1+const2*maxY))#
}#
#
# calculate two-sample sampling variance#
ds.var.2s <- function(treatment.vec,control.vec) ds.var(treatment.vec[1],treatment.vec[2],treatment.vec[3],treatment.vec[4],treatment.vec[5],treatment.vec[6],treatment.vec[7],treatment.vec[8]) + ds.var(control.vec[1],control.vec[2],control.vec[3],control.vec[4],control.vec[5],control.vec[6],control.vec[7],control.vec[8])#
#
# estimate manski bounds, two-sample:#
ds.manski.2s <- function(p1.t,p2.t,y1m.t,y2m.t,p1.c,p2.c,y1m.c,y2m.c,minY,maxY) {#
	const1.t <- p1.t*y1m.t + (1-p1.t)*p2.t*y2m.t#
	const2.t <- (1-p1.t)*(1-p2.t)#
	const1.c <- p1.c*y1m.c + (1-p1.c)*p2.c*y2m.c#
	const2.c <- (1-p1.c)*(1-p2.c)#
	return(c(const1.t+const2.t*minY - (const1.c+const2.c*maxY),const1.t+const2.t*maxY - (const1.c+const2.c*minY)))#
	}#
ds.manski.cis.2s <- function(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha) {#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
#
	y2m.t.L <- gen.mean(y2m.nm.t,p2.t,lower.bound=TRUE,minY,maxY)#
	y2m.t.U <- gen.mean(y2m.nm.t,p2.t,lower.bound=FALSE,minY,maxY)#
	y2m.c.L <- gen.mean(y2m.nm.c,p2.c,lower.bound=TRUE,minY,maxY)#
	y2m.c.U <- gen.mean(y2m.nm.c,p2.c,lower.bound=FALSE,minY,maxY)#
	s2.t.L <- gen.var(y2m.nm.t,s2.nm.t,p2.t,lower.bound=TRUE,minY,maxY)^.5#
	s2.t.U <- gen.var(y2m.nm.t,s2.nm.t,p2.t,lower.bound=FALSE,minY,maxY)^.5#
	s2.c.L <- gen.var(y2m.nm.c,s2.nm.t,p2.c,lower.bound=TRUE,minY,maxY)^.5#
	s2.c.U <- gen.var(y2m.nm.c,s2.nm.t,p2.c,lower.bound=FALSE,minY,maxY)^.5#
#
	manski.bounds.est <- ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
	lower.bound.est <- ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)[1]#
	upper.bound.est <- ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)[2]#
	lower.bound.var.est <- ds.var.2s(c(n1.t,n2.t,p1.t,p2.t,s1.t,s2.t.L,y1m.t,y2m.t.L),c(n1.c,n2.c,p1.c,p2.c,s1.c,s2.c.U,y1m.c,y2m.c.U))#
	upper.bound.var.est <- ds.var.2s(c(n1.t,n2.t,p1.t,p2.t,s1.t,s2.t.U,y1m.t,y2m.t.U),c(n1.c,n2.c,p1.c,p2.c,s1.c,s2.c.L,y1m.c,y2m.c.L))#
	 sig <- qnorm(1-(1-alpha)/2)#
	 return(c(lower.bound.est - sig*lower.bound.var.est^.5,upper.bound.est + sig*upper.bound.var.est^.5))#
}#
#
# find the n1,n2 (number of attempts to measure) that minimize width of Manski CIs#
optim.manski.2s <- function(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,starting.values = NULL,mu=1e-04) {#
	if( is.null(starting.values)) {#
	# cheap starting values#
	n1g.t = 5/p1.t#
	n2g.t = 4#
	n1g.c = 5/p1.c#
	n2g.c = 4} else {#
		n1g.t = starting.values[1]#
		n2g.t = starting.values[2]#
		n1g.c = starting.values[3]#
		n2g.c = starting.values[4]#
	}#
	# constraints#
    #- (c1a.t+c1r.t*p1.t)*n1.t - (c2a.t+c2r.t*p2.t)*n2.t - (c1a.c+c1r.c*p1.c)*n1.c - (c2a.c+c2r.c*p2.c)*n2.c + budget >= 0#
    # n1.t > 2   #
    # n2.t > 2 #
    # n1.c > 2   #
    # n2.c > 2 #
    # (1-p1.t)*n1.t - n2.t > 2#
    # (1-p1.c)*n1.c - n2.c > 2#
	# constraints#
    # - (c1a.t+c1r.t*p1.t)*n1 - (c2a.t+c2r.t*p2,t)*n2 + budget >= 0#
    # n1.t >= 2   #
    # n2.t >= 2 #
    # (1-p1.t)*n1.t - n2.t >= 2#
    # same as above, verify by c+p#
	constrMat <- rbind(c(-(c1a.t+c1r.t*p1.t),-(c2a.t+c2r.t*p2.t),-(c1a.c+c1r.c*p1.c),-(c2a.c+c2r.c*p2.c)),#
							c(1,0,0,0),#
							c(0,1,0,0),#
							c(0,0,1,0),#
							c(0,0,0,1),#
							c((1-p1.t),-1,0,0),#
							c(0,0,(1-p1.c),-1)#
							)#
	constrVec <- c(-budget,2,2,2,2,2,2)#
	# compute width of 95% CIs#
	ds.passthrough.2s <- function(ns,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha) diff(ds.manski.cis.2s(ns[1],ns[2],ns[3],ns[4],p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha))#
	# grad.passthrough.2s <- grad(ds.passthrough.2s,ns,method.args=list(ns,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha))#
	optimD <- constrOptim(c(n1g.t,n2g.t,n1g.c,n2g.c),ds.passthrough.2s,grad=NULL,ui=constrMat,ci=constrVec,p1.t=p1.t,p2.t=p2.t,s1.t=s1.t,s2.nm.t=s2.nm.t,y1m.t=y1m.t,y2m.nm.t=y2m.nm.t,c1a.t=c1a.t,c1r.t=c1r.t,c2a.t=c2a.t,c2r.t=c2r.t,p1.c=p1.c,p2.c=p2.c,s1.c=s1.c,s2.nm.c=s2.nm.c,y1m.c=y1m.c,y2m.nm.c=y2m.nm.c,minY=minY,maxY=maxY,alpha=alpha,outer.iterations=10000,outer.eps = .Machine$double.eps^.8,mu=mu)#
	return(c(n1.t = optimD$par[1],n2.t = optimD$par[2],n1.c = optimD$par[3],n2.c = optimD$par[4],ci.width=optimD$value))	#
}#
#
############
#
# Sample code -- 2s#
#
p1.t <- .10 # expected proportion nonmissing in first round#
p2.t <- .50 # expected proportion nonmissing in second round#
s1.t <- 2 # standard deviation in the first round#
s2.nm.t <- 5 # standard deviation in the second round#
y1m.t <- 5 # mean in the first round#
y2m.nm.t <- 5 # mean in the second round#
c1a.t <- .05 # cost of attempting to measure in first round#
c1r.t <- .10 # cost of measuring in first round (above attempting)#
c2a.t <- .10 # cost of attempting to measure in second round#
c2r.t <- 1 # cost of measuring in second round (above attempting)#
#
p1.c <- .10 # expected proportion nonmissing in first round#
p2.c <- .50 # expected proportion nonmissing in second round#
s1.c <- 2 # standard deviation in the first round#
s2.nm.c <- 5 # standard deviation in the second round#
y1m.c <- 5 # mean in the first round#
y2m.nm.c <- 5 # mean in the second round#
c1a.c <- .05 # cost of attempting to measure in first round#
c1r.c <- .10 # cost of measuring in first round (above attempting)#
c2a.c <- .10 # cost of attempting to measure in second round#
c2r.c <- 1 # cost of measuring in second round (above attempting)#
#
maxY <- 10#
minY <- 0#
alpha <- 0.90#
budget <- 100 # overall budget#
#
ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
#
find.opt.manski.2s <- optim.manski.2s(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,starting.values=NULL,mu=1e-05)#
#
find.opt.manski.2s#
#
n1.t <- find.opt.manski.2s[1]#
n2.t <- find.opt.manski.2s[2]#
n1.c <- find.opt.manski.2s[3]#
n2.c <- find.opt.manski.2s[4]#
#
ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)
source('~/Dropbox/Double sampling/sampling-package-7-9.R', chdir = TRUE)
source('~/Dropbox/Double sampling/sampling-package-7-9.R', chdir = TRUE)#
#
############
#
p1.t <- p1.c <- 		.75 # expected proportion nonmissing in first round#
p2.t <- p2.c <- 		.50 # expected proportion nonmissing in second round#
y1m.t <- y1m.c <- 		.5 # mean in the first round#
y2m.nm.t <- y2m.nm.c <- 	.25 # mean in the second round#
s1.t <- s1.c <- 		(y1m.t*(1-y1m.t))^.5 # standard deviation in the first round#
s2.nm.t <- s2.nm.c 	<- 	(y2m.nm.t*(1-y2m.nm.t))^.5 # standard deviation in the second round#
c1a.t <- c1a.c <- 		0 # cost of attempting to measure in first round#
c1r.t <- c1r.c <- 		0.10 # cost of measuring in first round (above attempting)#
c2a.t <- c2a.c <- 		0 # cost of attempting to measure in second round#
c2r.t <- c2r.c <- 		1.00 # cost of measuring in second round (above attempting)#
#
minY <- 0#
maxY <- 1#
alpha <- 0.95#
budget <- 100 # overall budget#
#
ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
#
find.opt.manski.2s <- optim.manski.2s(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,mu=5e-05,niter=100)#
#
(find.opt.manski.2s.1 <- find.opt.manski.2s[1,])#
#
n1.t <- find.opt.manski.2s.1[1]#
n2.t <- find.opt.manski.2s.1[2]#
n1.c <- find.opt.manski.2s.1[3]#
n2.c <- find.opt.manski.2s.1[4]#
#
ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)#
#
#### assumes symmetry, for simulations#
#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
# only using first sample#
#
n1s <- budget/(c1a.t+c1r.t*p1.t)#
#
opt.1s.lb.est <- gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY)#
opt.1s.var.est <- gen.var(y1m.t,s1.t,p1.t,lower.bound=FALSE,minY,maxY)*2/n1s + gen.var(y1m.t,s1.t,p1.t,lower.bound=TRUE,minY,maxY)*2/n1s#
#
opt.1s.ub.est <- gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY)#
#
(ci.1s <- c(opt.1s.lb.est-qnorm(.975)*opt.1s.var.est^.5,opt.1s.ub.est+qnorm(.975)*opt.1s.var.est^.5))#
#
# only using second sampling, assumes that second group can pick up everyone in the first group - very generous#
#
p2.A <- p1.t + p2.t*(1-p1.t)#
#
n2s <- budget/(c2a.t+c2r.t*p2.A)#
#
y2.A <- (y1m.t*p1.t + y2m.nm.t*p2.t*(1-p1.t))/(p1.t+p2.t*(1-p1.t)) # double check this#
s2.A <- y2.A*(1-y2.A)#
opt.2s.lb.est <- gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY)#
opt.2s.var.est <- gen.var(y2.A,s2.A,p2.A,lower.bound=FALSE,minY,maxY)*2/n2s + gen.var(y2.A,s2.A,p2.A,lower.bound=TRUE,minY,maxY)*2/n2s#
#
opt.2s.ub.est <- gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY)#
#
(ci.2s <- c(opt.2s.lb.est-qnorm(.975)*opt.2s.var.est^.5,opt.2s.ub.est+qnorm(.975)*opt.2s.var.est^.5))#
#
# generate table line:#
#
round(c(n.1st=n1s*p1.t,ci.1st=ci.1s,n.2nd=n2s*p2.A,ci.2nd=ci.2s,n.1st.ds=(n1.t+n1.c)*p1.t,n.2nd.ds=(n2.t+n2.c)*p2.t,ci.ds=ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)),digits=3)
source('~/Dropbox/Double sampling/sampling-package-7-9.R', chdir = TRUE)#
#
############
#
p1.t <- p1.c <- 		.5  # expected proportion nonmissing in first round#
p2.t <- p2.c <- 		.50 # expected proportion nonmissing in second round#
y1m.t <- y1m.c <- 		.5 # mean in the first round#
y2m.nm.t <- y2m.nm.c <- 	.25 # mean in the second round#
s1.t <- s1.c <- 		(y1m.t*(1-y1m.t))^.5 # standard deviation in the first round#
s2.nm.t <- s2.nm.c 	<- 	(y2m.nm.t*(1-y2m.nm.t))^.5 # standard deviation in the second round#
c1a.t <- c1a.c <- 		0 # cost of attempting to measure in first round#
c1r.t <- c1r.c <- 		0.10 # cost of measuring in first round (above attempting)#
c2a.t <- c2a.c <- 		0 # cost of attempting to measure in second round#
c2r.t <- c2r.c <- 		1.00 # cost of measuring in second round (above attempting)#
#
minY <- 0#
maxY <- 1#
alpha <- 0.95#
budget <- 100 # overall budget#
#
ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
#
find.opt.manski.2s <- optim.manski.2s(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,mu=5e-05,niter=100)#
#
(find.opt.manski.2s.1 <- find.opt.manski.2s[1,])#
#
n1.t <- find.opt.manski.2s.1[1]#
n2.t <- find.opt.manski.2s.1[2]#
n1.c <- find.opt.manski.2s.1[3]#
n2.c <- find.opt.manski.2s.1[4]#
#
ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)#
#
#### assumes symmetry, for simulations#
#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
# only using first sample#
#
n1s <- budget/(c1a.t+c1r.t*p1.t)#
#
opt.1s.lb.est <- gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY)#
opt.1s.var.est <- gen.var(y1m.t,s1.t,p1.t,lower.bound=FALSE,minY,maxY)*2/n1s + gen.var(y1m.t,s1.t,p1.t,lower.bound=TRUE,minY,maxY)*2/n1s#
#
opt.1s.ub.est <- gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY)#
#
(ci.1s <- c(opt.1s.lb.est-qnorm(.975)*opt.1s.var.est^.5,opt.1s.ub.est+qnorm(.975)*opt.1s.var.est^.5))#
#
# only using second sampling, assumes that second group can pick up everyone in the first group - very generous#
#
p2.A <- p1.t + p2.t*(1-p1.t)#
#
n2s <- budget/(c2a.t+c2r.t*p2.A)#
#
y2.A <- (y1m.t*p1.t + y2m.nm.t*p2.t*(1-p1.t))/(p1.t+p2.t*(1-p1.t)) # double check this#
s2.A <- y2.A*(1-y2.A)#
opt.2s.lb.est <- gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY)#
opt.2s.var.est <- gen.var(y2.A,s2.A,p2.A,lower.bound=FALSE,minY,maxY)*2/n2s + gen.var(y2.A,s2.A,p2.A,lower.bound=TRUE,minY,maxY)*2/n2s#
#
opt.2s.ub.est <- gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY)#
#
(ci.2s <- c(opt.2s.lb.est-qnorm(.975)*opt.2s.var.est^.5,opt.2s.ub.est+qnorm(.975)*opt.2s.var.est^.5))#
#
# generate table line:#
#
round(c(n.1st=n1s*p1.t,ci.1st=ci.1s,n.2nd=n2s*p2.A,ci.2nd=ci.2s,n.1st.ds=(n1.t+n1.c)*p1.t,n.2nd.ds=(n2.t+n2.c)*p2.t,ci.ds=ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)),digits=3)
source('~/Dropbox/Double sampling/sampling-package-7-9.R', chdir = TRUE)#
#
############
#
p1.t <- p1.c <- 		.5  # expected proportion nonmissing in first round#
p2.t <- p2.c <- 		.50 # expected proportion nonmissing in second round#
y1m.t <- y1m.c <- 		.5 # mean in the first round#
y2m.nm.t <- y2m.nm.c <- 	.25 # mean in the second round#
s1.t <- s1.c <- 		(y1m.t*(1-y1m.t))^.5 # standard deviation in the first round#
s2.nm.t <- s2.nm.c 	<- 	(y2m.nm.t*(1-y2m.nm.t))^.5 # standard deviation in the second round#
c1a.t <- c1a.c <- 		0 # cost of attempting to measure in first round#
c1r.t <- c1r.c <- 		0.10 # cost of measuring in first round (above attempting)#
c2a.t <- c2a.c <- 		0 # cost of attempting to measure in second round#
c2r.t <- c2r.c <- 		10.00 # cost of measuring in second round (above attempting)#
#
minY <- 0#
maxY <- 1#
alpha <- 0.95#
budget <- 100 # overall budget#
#
ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
#
find.opt.manski.2s <- optim.manski.2s(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,mu=5e-05,niter=100)#
#
(find.opt.manski.2s.1 <- find.opt.manski.2s[1,])#
#
n1.t <- find.opt.manski.2s.1[1]#
n2.t <- find.opt.manski.2s.1[2]#
n1.c <- find.opt.manski.2s.1[3]#
n2.c <- find.opt.manski.2s.1[4]#
#
ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)#
#
#### assumes symmetry, for simulations#
#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
# only using first sample#
#
n1s <- budget/(c1a.t+c1r.t*p1.t)#
#
opt.1s.lb.est <- gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY)#
opt.1s.var.est <- gen.var(y1m.t,s1.t,p1.t,lower.bound=FALSE,minY,maxY)*2/n1s + gen.var(y1m.t,s1.t,p1.t,lower.bound=TRUE,minY,maxY)*2/n1s#
#
opt.1s.ub.est <- gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY)#
#
(ci.1s <- c(opt.1s.lb.est-qnorm(.975)*opt.1s.var.est^.5,opt.1s.ub.est+qnorm(.975)*opt.1s.var.est^.5))#
#
# only using second sampling, assumes that second group can pick up everyone in the first group - very generous#
#
p2.A <- p1.t + p2.t*(1-p1.t)#
#
n2s <- budget/(c2a.t+c2r.t*p2.A)#
#
y2.A <- (y1m.t*p1.t + y2m.nm.t*p2.t*(1-p1.t))/(p1.t+p2.t*(1-p1.t)) # double check this#
s2.A <- y2.A*(1-y2.A)#
opt.2s.lb.est <- gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY)#
opt.2s.var.est <- gen.var(y2.A,s2.A,p2.A,lower.bound=FALSE,minY,maxY)*2/n2s + gen.var(y2.A,s2.A,p2.A,lower.bound=TRUE,minY,maxY)*2/n2s#
#
opt.2s.ub.est <- gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY)#
#
(ci.2s <- c(opt.2s.lb.est-qnorm(.975)*opt.2s.var.est^.5,opt.2s.ub.est+qnorm(.975)*opt.2s.var.est^.5))#
#
# generate table line:#
#
round(c(n.1st=n1s*p1.t,ci.1st=ci.1s,n.2nd=n2s*p2.A,ci.2nd=ci.2s,n.1st.ds=(n1.t+n1.c)*p1.t,n.2nd.ds=(n2.t+n2.c)*p2.t,ci.ds=ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)),digits=3)
source('~/Dropbox/Double sampling/sampling-package-7-9.R', chdir = TRUE)#
#
############
#
p1.t <- p1.c <- 		.75  # expected proportion nonmissing in first round#
p2.t <- p2.c <- 		.80 # expected proportion nonmissing in second round#
y1m.t <- y1m.c <- 		.5 # mean in the first round#
y2m.nm.t <- y2m.nm.c <- 	.25 # mean in the second round#
s1.t <- s1.c <- 		(y1m.t*(1-y1m.t))^.5 # standard deviation in the first round#
s2.nm.t <- s2.nm.c 	<- 	(y2m.nm.t*(1-y2m.nm.t))^.5 # standard deviation in the second round#
c1a.t <- c1a.c <- 		0 # cost of attempting to measure in first round#
c1r.t <- c1r.c <- 		0.10 # cost of measuring in first round (above attempting)#
c2a.t <- c2a.c <- 		0 # cost of attempting to measure in second round#
c2r.t <- c2r.c <- 		10.00 # cost of measuring in second round (above attempting)#
#
minY <- 0#
maxY <- 1#
alpha <- 0.95#
budget <- 100 # overall budget#
#
ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
#
find.opt.manski.2s <- optim.manski.2s(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,mu=5e-05,niter=100)#
#
(find.opt.manski.2s.1 <- find.opt.manski.2s[1,])#
#
n1.t <- find.opt.manski.2s.1[1]#
n2.t <- find.opt.manski.2s.1[2]#
n1.c <- find.opt.manski.2s.1[3]#
n2.c <- find.opt.manski.2s.1[4]#
#
ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)#
#
#### assumes symmetry, for simulations#
#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
# only using first sample#
#
n1s <- budget/(c1a.t+c1r.t*p1.t)#
#
opt.1s.lb.est <- gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY)#
opt.1s.var.est <- gen.var(y1m.t,s1.t,p1.t,lower.bound=FALSE,minY,maxY)*2/n1s + gen.var(y1m.t,s1.t,p1.t,lower.bound=TRUE,minY,maxY)*2/n1s#
#
opt.1s.ub.est <- gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY)#
#
(ci.1s <- c(opt.1s.lb.est-qnorm(.975)*opt.1s.var.est^.5,opt.1s.ub.est+qnorm(.975)*opt.1s.var.est^.5))#
#
# only using second sampling, assumes that second group can pick up everyone in the first group - very generous#
#
p2.A <- p1.t + p2.t*(1-p1.t)#
#
n2s <- budget/(c2a.t+c2r.t*p2.A)#
#
y2.A <- (y1m.t*p1.t + y2m.nm.t*p2.t*(1-p1.t))/(p1.t+p2.t*(1-p1.t)) # double check this#
s2.A <- y2.A*(1-y2.A)#
opt.2s.lb.est <- gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY)#
opt.2s.var.est <- gen.var(y2.A,s2.A,p2.A,lower.bound=FALSE,minY,maxY)*2/n2s + gen.var(y2.A,s2.A,p2.A,lower.bound=TRUE,minY,maxY)*2/n2s#
#
opt.2s.ub.est <- gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY)#
#
(ci.2s <- c(opt.2s.lb.est-qnorm(.975)*opt.2s.var.est^.5,opt.2s.ub.est+qnorm(.975)*opt.2s.var.est^.5))#
#
# generate table line:#
#
round(c(n.1st=n1s*p1.t,ci.1st=ci.1s,n.2nd=n2s*p2.A,ci.2nd=ci.2s,n.1st.ds=(n1.t+n1.c)*p1.t,n.2nd.ds=(n2.t+n2.c)*p2.t,ci.ds=ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)),digits=3)
source('~/Dropbox/Double sampling/sampling-package-7-9.R', chdir = TRUE)#
#
############
#
p1.t <- p1.c <- 		.75  # expected proportion nonmissing in first round#
p2.t <- p2.c <- 		.80 # expected proportion nonmissing in second round#
y1m.t <- y1m.c <- 		.5 # mean in the first round#
y2m.nm.t <- y2m.nm.c <- 	.25 # mean in the second round#
s1.t <- s1.c <- 		(y1m.t*(1-y1m.t))^.5 # standard deviation in the first round#
s2.nm.t <- s2.nm.c 	<- 	(y2m.nm.t*(1-y2m.nm.t))^.5 # standard deviation in the second round#
c1a.t <- c1a.c <- 		0 # cost of attempting to measure in first round#
c1r.t <- c1r.c <- 		0.10 # cost of measuring in first round (above attempting)#
c2a.t <- c2a.c <- 		0 # cost of attempting to measure in second round#
c2r.t <- c2r.c <- 		10.00 # cost of measuring in second round (above attempting)#
#
minY <- 0#
maxY <- 1#
alpha <- 0.95#
budget <- 1000 # overall budget#
#
ds.manski.2s(p1.t,p2.t,y1m.t,y2m.nm.t,p1.c,p2.c,y1m.c,y2m.nm.c,minY,maxY)#
#
find.opt.manski.2s <- optim.manski.2s(p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,c1a.c,c1r.c,c2a.c,c2r.c,budget,minY,maxY,mu=5e-05,niter=100)#
#
(find.opt.manski.2s.1 <- find.opt.manski.2s[1,])#
#
n1.t <- find.opt.manski.2s.1[1]#
n2.t <- find.opt.manski.2s.1[2]#
n1.c <- find.opt.manski.2s.1[3]#
n2.c <- find.opt.manski.2s.1[4]#
#
ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)#
#
#### assumes symmetry, for simulations#
#
	# compute 2nd round manski mean#
	gen.mean <- function(y2m.nm,p2,lower.bound=TRUE,minY,maxY) if (lower.bound == TRUE) return(p2*y2m.nm + (1-p2)*minY) else return(p2*y2m.nm+(1-p2)*maxY)#
	# compute 2nd round manski sd given prop, which bound, and mean#
	gen.var <- function(y2m.nm,s2.nm,p2,lower.bound=TRUE,minY,maxY) {#
		if (lower.bound==TRUE) const <- minY else const <- maxY#
		wm <- gen.mean(y2m.nm,p2,lower.bound,minY,maxY)#
		return(#
	    (p2*s2.nm^2 + p2*(y2m.nm-wm)^2 + (1-p2)*(const-wm)^2)#
		) # formula for combined var#
	}#
# only using first sample#
#
n1s <- budget/(c1a.t+c1r.t*p1.t)#
#
opt.1s.lb.est <- gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY)#
opt.1s.var.est <- gen.var(y1m.t,s1.t,p1.t,lower.bound=FALSE,minY,maxY)*2/n1s + gen.var(y1m.t,s1.t,p1.t,lower.bound=TRUE,minY,maxY)*2/n1s#
#
opt.1s.ub.est <- gen.mean(y1m.t,p1.t,lower.bound=FALSE,minY,maxY) - gen.mean(y1m.t,p1.t,lower.bound=TRUE,minY,maxY)#
#
(ci.1s <- c(opt.1s.lb.est-qnorm(.975)*opt.1s.var.est^.5,opt.1s.ub.est+qnorm(.975)*opt.1s.var.est^.5))#
#
# only using second sampling, assumes that second group can pick up everyone in the first group - very generous#
#
p2.A <- p1.t + p2.t*(1-p1.t)#
#
n2s <- budget/(c2a.t+c2r.t*p2.A)#
#
y2.A <- (y1m.t*p1.t + y2m.nm.t*p2.t*(1-p1.t))/(p1.t+p2.t*(1-p1.t)) # double check this#
s2.A <- y2.A*(1-y2.A)#
opt.2s.lb.est <- gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY)#
opt.2s.var.est <- gen.var(y2.A,s2.A,p2.A,lower.bound=FALSE,minY,maxY)*2/n2s + gen.var(y2.A,s2.A,p2.A,lower.bound=TRUE,minY,maxY)*2/n2s#
#
opt.2s.ub.est <- gen.mean(y2.A,p2.A,lower.bound=FALSE,minY,maxY) - gen.mean(y2.A,p2.A,lower.bound=TRUE,minY,maxY)#
#
(ci.2s <- c(opt.2s.lb.est-qnorm(.975)*opt.2s.var.est^.5,opt.2s.ub.est+qnorm(.975)*opt.2s.var.est^.5))#
#
# generate table line:#
#
round(c(n.1st=n1s*p1.t,ci.1st=ci.1s,n.2nd=n2s*p2.A,ci.2nd=ci.2s,n.1st.ds=(n1.t+n1.c)*p1.t,n.2nd.ds=(n2.t+n2.c)*p2.t,ci.ds=ds.manski.cis.2s(n1.t,n2.t,n1.c,n2.c,p1.t,p2.t,s1.t,s2.nm.t,y1m.t,y2m.nm.t,c1a.t,c1r.t,c2a.t,c2r.t,p1.c,p2.c,s1.c,s2.nm.c,y1m.c,y2m.nm.c,minY,maxY,alpha)),digits=3)
x1=rnorm(100000,0,1)
x2=rnorm(100000,0,1)
x3=rnorm(100000,0,1)
sum=x1+x2+x3
length(sum[sum > (.8+.6+1.2)])
length(sum[sum > (.8+.6+1.2)])/length(sum)
length(sum[sum > (.86+.65+1.22)])/length(sum)
x1 <- rnorm(100000,0,1)
x2 <- rnorm(100000,0,1)
x3 <- rnorm(100000,0,1)
length(x1[x1 > 1.2])
.388/2 * .515/2 * .221/2
settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .75*Ypre # true cluster means post, assuming no effect#
niter <- 100#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(1234567)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .75*Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
N
Ypre
sortc(Ypre)
sort(Ypre)
n.treat
little.n
Zseed
mean(Zseed)
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after
gen.se(100,300) # SE given 20 per cluster before, 80 per cluster after
gen.se(300,100) # SE given 20 per cluster before, 80 per cluster after
simulation settings#
#
set.seed(1234567)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .75*Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(1234567)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
optim.prop
gen.se(49,51)
gen.se(51,49)
simulation settings#
#
set.seed(1234567)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .75*Ypre # true cluster means post, assuming no effect#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(1234567)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
simulation settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .75*Ypre # true cluster means post, assuming no effect#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
simulation settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .75*Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
simulation settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .5*Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
simulation settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25 + .05*Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
Ypost <- .25a + 1*Ypre # true cluster means post, assuming no effect
simulation settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- .25a + 1*Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
simulation settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- runif(40)#
#Ypost <- .25 + 1*Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
simulation settings#
#
set.seed(123)#
#
Ypre <- runif(40) # true cluster means pre#
Ypost <- runif(40)#
#Ypost <- .25 + 1*Ypre # true cluster means post, assuming no effect#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat)) == 1#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
set.seed(123)#
#
tauest <- rep(NA,niter)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	tauest[i] <- mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
#
gen.se(20,80) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 1#
		nafter <- nafter - 1#
	}#
	if(nafter == 0) {#
		nafter <- 1#
		nbefore <- nbefore - 1#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
gen.se(1,99)
settings#
#
set.seed(123)#
#
Ypre <- runif(30) # true cluster means pre#
Ypost <- runif(30) # true cluster means post, assuming no effect#
#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(50,50) # SE given 20 per cluster before, 80 per cluster after#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
# optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
# optim.prop#
#
# round(n.afford*optim.prop$minimum) # optimal before#
# round(n.afford*(1-optim.prop$minimum)) # optimal after
Ypre
lm(Ypost ~ Ypre)
optim.prop
optim.prop <- optimize(optim.fn,c(0.01,.99))#
#
 optim.prop
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(123)#
#
Ypre <- runif(30) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect#
Ypost <- Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(50,50) # SE given 20 per cluster before, 80 per cluster after#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
runif(10,.2,.5)
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect#
Ypost <- Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(50,50) # SE given 20 per cluster before, 80 per cluster after#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 100 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect#
Ypost <- Ypre # true cluster means post, assuming no effect#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect#
Ypost <- Ypre # true cluster means post, assuming no effect#
niter <- 2000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
sd(Ypre)
sd(Ypost)
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
Ypost <- .3+.9*Ypre+rnorm(30,0,.1) # true cluster means post, assuming no effect
sd(Ypre)
sd(Ypost)
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
Ypost <- .3+.8*Ypre+rnorm(30,0,.1) # true cluster means post, assuming no effect
sd(Ypost)
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, assuming no effect
sd(Ypost)
corr(Ypre,Ypost)
cor(Ypre,Ypost)
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
#Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 2000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
#Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
mean(Ypre)
mean(Ypost)
Ypost <- .3+.8*Ypre+rnorm(30,0,.05)
mean(Ypost)
plot(Ypre,Ypost)
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 40 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 1000 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
c(1,2,3)
Ypre
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)
Ypre
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 100#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)
length(Ypre)
length(Ypost)
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
#Ypre <- runif(30,.2,.4) # true cluster means pre#
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 1000#
n.treat <- 10
some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
Ypre <- runif(30,.2,.4) # true cluster means pre#
#Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)#
Ypre <- Ypre/100#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
Ypre
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
#Ypre <- runif(30,.2,.4) # true cluster means pre#
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)#
Ypre <- Ypre/100#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 1000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
#Ypre <- runif(30,.2,.4) # true cluster means pre#
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)  #actual, ignores blocking#
Ypre <- Ypre/100#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
Ypre <- runif(15,.2,.4) # true cluster means pre#
#Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)  #actual, ignores blocking#
#Ypre <- Ypre/100#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 1000#
n.treat <- 5#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
Ypre <- runif(9,.2,.4) # true cluster means pre#
#Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)  #actual, ignores blocking#
#Ypre <- Ypre/100#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 3#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
Ypre <- runif(90,.2,.4) # true cluster means pre#
#Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)  #actual, ignores blocking#
#Ypre <- Ypre/100#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 30#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
> plot(optimvals~seq(0.05,0.95,0.025))
plot(optimvals~seq(0.05,0.95,0.025))
settings#
rm(list=ls())       # clear objects in memory#
#
set.seed(123)#
#
#Ypre <- runif(30,.2,.4) # true cluster means pre#
Ypre <-c(33.05,32.07,31.45,30.95,30.68,30.00,29.21,27.66,26.87,26.44,26.43,26.09,25.52,25.20,25.00,25.00,24.18,24.12,23.81,23.35,23.26,22.61,22.00,21.89,21.37,21.24,19.67,18.37,17.54,15.38)  #actual, ignores blocking#
Ypre <- Ypre/100#
#
#Ypost <- runif(30) # true cluster means post, assuming no effect, and no prognostic value#
Ypost <- Ypre # true cluster means post, assuming no effect, and R2 of 1#
#Ypost <- .3+.8*Ypre+rnorm(30,0,.05) # true cluster means post, R2 of ~.6#
niter <- 10000#
n.treat <- 10#
#
#### some convenience calculations#
#
N <- length(Ypre)#
genclumean <- function(y,little.n) mean(rbinom(little.n,1,y))#
Zseed <- c(rep(1,n.treat),rep(0,N-n.treat))#
#
#####
#
# generate appx SE function#
#
gen.se <- function(nbefore,nafter) {#
tauest <- rep(NA,niter)#
#
set.seed(123)#
#
for(i in 1:niter) {#
	Ypresim <- sapply(Ypre,genclumean,little.n=nbefore)#
	Ypostsim <- sapply(Ypost,genclumean,little.n=nafter)#
	Ydiffsim <- Ypostsim - Ypresim#
	Z <- sample(Zseed)#
	Xvec <- cbind(1,Z,Ypresim)#
	tauest[i] <- (solve(t(Xvec) %*% Xvec) %*% t(Xvec) %*% Ypostsim)[2]#
	#mean(Ydiffsim[Z]) - mean(Ydiffsim[!Z]) # subtracting off pretest#
}#
return(sd(tauest))#
}#
gen.se(30,70) # SE given 20 per cluster before, 80 per cluster after#
#
##### optimization#
#
n.afford <- 400 # number of interviews that we can afford per cluster#
#
optim.fn <- function(proportion.before) {#
	nbefore <- round(n.afford*proportion.before)#
	nafter <- round(n.afford*(1-proportion.before))#
	# dealing with boundary conditions#
	if(nbefore == 0) { #
		nbefore <- 2#
		nafter <- nafter - 2#
	}#
	if(nafter == 0) {#
		nafter <- 2#
		nbefore <- nbefore - 2#
	}#
	return(gen.se(nbefore,nafter))#
}#
#
# se.plot#
#
optimvals <- sapply(seq(0.05,0.95,0.025),optim.fn)#
#
plot(optimvals~seq(0.05,0.95,0.025))#
#
optim.prop <- optimize(optim.fn,c(0.01,.99))#
optim.prop#
#
round(n.afford*optim.prop$minimum) # optimal before#
round(n.afford*(1-optim.prop$minimum)) # optimal after
plot(optimvals~seq(0.05,0.95,0.025))
r<- .7
(1-r)/(1-r^2)
r<- 0
(1-r)/(1-r^2)
r<- .99
(1-r)/(1-r^2)
r<- .99999
(1-r)/(1-r^2)
cor(Ypre,Ypost)
r<- .8
cor(Ypre,Ypost)
(1-r)/(1-r^2)
Artificial example to illustrate tree models#
library(BayesTree) rm(list = ls(all = TRUE)) set.seed(123456) n <- 1000 Tr <- rbinom(n,1,.5) x <- runif(n,0,3) y0 <- rnorm(n,0,0.5) y1 <- y0 + Tr*(x*(x<=1)+(x>1)*(x-2)^2) + rnorm(n,0,.5) + .2 y <- y0 y[Tr==1] <- y1[Tr==1]#
dip <- function(x){ (x<=1)*x + (x>1)*(x-2)^2 }
library(BayesTree)
rm(list = ls(all = TRUE))
set.seed(123456) n <- 1000 Tr <- rbinom(n,1,.5) x <- runif(n,0,3) y0 <- rnorm(n,0,0.5) y1 <- y0 + Tr*(x*(x<=1)+(x>1)*(x-2)^2) + rnorm(n,0,.5) + .2 y <- y0 y[Tr==1] <- y1[Tr==1]#
dip <- function(x){ (x<=1)*x + (x>1)*(x-2)^2 }
set.seed(123456)
n <- 1000
Tr <- rbinom(n,1,.5)
x <- runif(n,0,3)
y0 <- rnorm(n,0,0.5)
y1 <- y0 + Tr*(x*(x<=1)+(x>1)*(x-2)^2) + rnorm(n,0,.5) + .2 y <- y0
y1 <- y0 + Tr*(x*(x<=1)+(x>1)*(x-2)^2) + rnorm(n,0,.5) + .2 *y <- y0
y1 <- y0 + Tr*(x*(x<=1)+(x>1)*(x-2)^2) + rnorm(n,0,.5) + .2
y <- y0
y[Tr==1] <- y1[Tr==1]
dip <- function(x){ (x<=1)*x + (x>1)*(x-2)^2 }
temp.X <- data.frame(Tr,x)
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x))) colnames(temp.S) <- colnames(temp.X)
temp.X <- data.frame(Tr,x)
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x))) colnames(temp.S) <- colnames(temp.X)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(153)
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,2)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,1)#
y <- y0#
y[Tr==1] <- y1[Tr==1]
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2
OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)
BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")
out.bart0 <- out.bart#
out.bart1 <- out.bart
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }
Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree
plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")
plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
set.seed(153)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,4)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)#
.
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(153)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,8)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,4)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,8)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,4)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,8)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,6)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,8)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,8)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,12)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,12)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
test <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))
test
?predict
data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)
colnames(temp.S)
colnames(temp.X)
length(temp.S)
dims(temp.S)
dim(temp.S)
dim(temp.X)
dim(y)
length(y)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,12)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,12)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 50,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,12)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,12)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 10,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,12)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,12)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 5,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,12)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,12)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 3,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,12)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,12)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 1,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
windows()#
set.seed(1234567)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,12)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,12)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 500,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
cat(dim(out.bart$yhat.test), "\n")
dim(out.bart$yhat.test)
out <- matrix(NA,n,3)#
out[,1] <- sort(x)
out
paste("GSS", covar, ".RData", sep = "")
covar <- "educ"#
part <- 1
paste("GSS", covar, ".RData", sep = "")
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 200#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,4)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 300#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,4)#
y1 <- y0 + Tr*x^2 + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
/*#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
*/#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 300#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,4)#
y1 <- y0 + Tr*x^3 + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
sin(x)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 300#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,4)#
y1 <- y0 + Tr*sin(x) + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 300#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x^2,4)#
y1 <- y0 + Tr*5*sin(x) + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 300#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x,4)#
y1 <- y0 + Tr*10*sin(x) + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 400#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x,4)#
y1 <- 5 + Tr*10*sin(x) + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
Artificial example to illustrate tree models#
### Using R 2.13.0#
library(tree)#
library(BayesTree)#
rm(list = ls(all = TRUE))#
#setwd("C:/Users/Holger/Documents/Dropbox/Current projects/Machine Learning/")#
#windows()#
set.seed(153)#
n <- 400#
Tr <- rbinom(n,1,.5)#
x <- rnorm(n,4,1)#
y0 <- rnorm(n,x,4)#
y1 <- 5 + Tr*sin(3*x) + rnorm(n,0,2)#
y <- y0#
y[Tr==1] <- y1[Tr==1]#
dat <- data.frame(y,Tr,x)#
temp <- lm(y ~ Tr*x, data = dat, x = TRUE, model = TRUE)#
summary(temp)#
# true response surfaces#
y0.true <- matrix(NA,1000,2)#
y0.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y0.true[,2] <- y0.true[,1]^2#
y1.true <- matrix(NA,1000,2)#
y1.true[,1] <- seq(from = min(x), to = max(x), length.out = 1000)#
y1.true[,2] <- y0.true[,2] + y1.true[,1]^2#
# OLS fit#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict(temp, newdata = dat)#
# lines(x = y1.true[,1], y = pred, lty = 2, lwd = 1.5)#
# BART fit#
temp.X <- data.frame(Tr,x)#
temp.S <- data.frame(c(rep(1,n),rep(0,n)), c(sort(x),sort(x)))#
colnames(temp.S) <- colnames(temp.X)#
out.bart <- bart(x.train = temp.X, y.train = y, ndpost = 1000, nskip = 1000, keepevery = 1,#
                ntree = 200,#
                usequants = TRUE, keeptrainfits = FALSE, x.test = temp.S)#
cat(dim(out.bart$yhat.test), "\n")#
out.bart0 <- out.bart#
out.bart1 <- out.bart#
out.bart1$yhat.test <- out.bart$yhat.test[,1:(ncol(out.bart$yhat.test)/2)]#
out.bart0$yhat.test <- out.bart$yhat.test[,((ncol(out.bart$yhat.test)/2)+1):ncol(out.bart$yhat.test)]#
out <- matrix(NA,n,3)#
out[,1] <- sort(x)#
colnames(out) <- c("x value", "Y0", "Y1")#
for(i in 1:n)   {#
    out[i,2] <- mean(out.bart0$yhat.test[,i])#
    out[i,3] <- mean(out.bart1$yhat.test[,i])#
                }#
# Add single tree fit#
temp.tree <- tree(y ~ Tr + x, mincut = 25)#
summary(temp.tree)#
plot(temp.tree)#
temp.tree#
# plot#
plot(x[Tr == 0], y[Tr == 0], ylim = range(y), xlim = range(x), ylab = "Y", xlab = "X", pch = 21,#
bg = "white", cex = 1)#
points(x[Tr == 1], y[Tr == 1], col = "black", pch = 24, bg = "white", cex = .8)#
lines(x = y0.true[,1], y = y0.true[,2], lty = 1, lwd = 2)#
lines(x = y1.true[,1], y = y1.true[,2], lty = 1, lwd = 2)#
lines(x = out[,1], y = out[,2], lty = 2, lwd = 2, col = "red")#
lines(x = out[,1], y = out[,3], lty = 2, lwd = 2, col = "red")#
dat <- data.frame(0, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
dat <- data.frame(1, seq(from = min(x), to = max(x), length.out = 1000))#
colnames(dat) <- c("Tr","x")#
pred <- predict.tree(temp.tree, newdata = dat)#
lines(x = y1.true[,1], y = pred, lty = 3, lwd = 2, col = "blue")#
# add legend#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(75,10), lty = 1, lwd = 2)#
text(x = 2.3, y = 75, "True response curve", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(70,10), lty = 2, lwd = 2, col = "red")#
text(x = 2.3, y = 70, "BART fit", pos = 4, cex = .8)#
lines(  x = seq(from = 2, to = 2.3, length.out = 10),#
        y = rep(65,10), lty = 3, lwd = 2, col = "blue")#
text(x = 2.3, y = 65, "Single tree fit", pos = 4, cex = .8)#
points(x = 2, y = 55, col = "black", pch = 21, bg = "white", cex = 1)#
text(x = 2, y = 55, "Control obs", pos = 4, cex = .8)#
points(x = 2, y = 60, col = "black", pch = 24, bg = "white", cex = .8)#
text(x = 2, y = 60, "Treated obs", pos = 4, cex = .8)
rm(list=ls())       # clear objects in memory#
library(ri)       # load the RI package#
library(foreign)    # package allows R to read Stata datasets#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
# Data are from Arceneaux, Kevin. 2005. “Using Cluster Randomized Field Experiments to Study Voting Behavior.” The Annals of the American Academy of Political and Social Science 601: 169-79.#
#
# read in data using Stata file - loaded directly from website.#
kansas <- read.dta("http://hdl.handle.net/10079/r2280ss")#
#
# Alternatively, you can instead read in data using .csv file#
# kansas <- read.csv(file="http://hdl.handle.net/10079/dr7sr5q",head=TRUE,sep=",")#
#
#redefine variables#
Z <-  kansas$treatmen#
Y <- kansas$vote03#
clust <- kansas$unit#
#
covs <- as.matrix(kansas[,2:21])  # covariates are past voter turnout if you care to perform a randomization check#
#
probs <- genprobexact(Z,clustvar=clust)  # subjects are clustered by precinct#
#
numiter <- 1000  # actual number of randomizations in this case is 40116600#
#
perms <- genperms(Z,maxiter=numiter,clustvar=clust)    # clustered assignment#
numiter <- ncol(perms)  # reset numiter so that it is no larger than the maximum number of possible randomizations#
#
## show the number of observations#
nrow(as.matrix(Y))#
#
## sort the data by cluster size in order to see the range#
sort(as.matrix((table(clust))))
Part (b) #
# Estimate the ATE using (possibly biased) difference-in-means#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                                  # estimated ATE#
sum(distout <= ate)                  # one-tailed comparison#
sum(abs(distout) >= abs(ate))        # two-tailed comparison#
dispdist(distout,ate)                #
#
# estimate the confidence interval assuming that the true ATE=ate#
Ys <- genouts(Y,Z,ate=ate)#
distout <- gendist(Ys,perms,prob=probs)#
dispdist(distout,ate)
Estimate the ATE using (unbiased but imprecise) difference-in-totals#
ateHT <- estate(Y,Z,prob=probs,HT=TRUE)    # Horvitz-Thompson difference-in-totals estimator#
distoutHT <- gendist(Ys,perms,prob=probs,HT=TRUE)#
#
ateHT                                # estimated difference-in-totals#
sum(distoutHT <= ateHT)             #
sum(abs(distoutHT) >= abs(ateHT))#
#
dispdist(distoutHT,ateHT)            # compare to null distribution
rm(list=ls())       # clear objects in memory#
library(ri)       # load the RI package#
library(foreign)    # package allows R to read Stata datasets#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
# Data are from Arceneaux, Kevin. 2005. “Using Cluster Randomized Field Experiments to Study Voting Behavior.” The Annals of the American Academy of Political and Social Science 601: 169-79.#
#
# read in data using Stata file - loaded directly from website.#
kansas <- read.dta("http://hdl.handle.net/10079/r2280ss")#
#
# Alternatively, you can instead read in data using .csv file#
# kansas <- read.csv(file="http://hdl.handle.net/10079/dr7sr5q",head=TRUE,sep=",")#
#
#redefine variables#
Z <-  kansas$treatmen#
Y <- kansas$vote03#
clust <- kansas$unit#
#
covs <- as.matrix(kansas[,2:21])  # covariates are past voter turnout if you care to perform a randomization check#
#
probs <- genprobexact(Z,clustvar=clust)  # subjects are clustered by precinct#
#
numiter <- 1000  # actual number of randomizations in this case is 40116600#
#
perms <- genperms(Z,maxiter=numiter,clustvar=clust)    # clustered assignment#
numiter <- ncol(perms)  # reset numiter so that it is no larger than the maximum number of possible randomizations#
#
## show the number of observations#
nrow(as.matrix(Y))#
#
## sort the data by cluster size in order to see the range#
sort(as.matrix((table(clust))))
ateHT <- estate(Y,Z,prob=probs,HT=TRUE)    # Horvitz-Thompson difference-in-totals estimator#
distoutHT <- gendist(Ys,perms,prob=probs,HT=TRUE)#
#
ateHT                                # estimated difference-in-totals#
sum(distoutHT <= ateHT)             #
sum(abs(distoutHT) >= abs(ateHT))#
#
dispdist(distoutHT,ateHT)            # compare to null distribution
Ys <- genouts(Y,Z,ate=0)
ateHT <- estate(Y,Z,prob=probs,HT=TRUE)    # Horvitz-Thompson difference-in-totals estimator#
distoutHT <- gendist(Ys,perms,prob=probs,HT=TRUE)#
#
ateHT                                # estimated difference-in-totals#
sum(distoutHT <= ateHT)             #
sum(abs(distoutHT) >= abs(ateHT))#
#
dispdist(distoutHT,ateHT)            # compare to null distribution
Chapter 3: Clustered Analysis -- example using Kansas City Clustered Design#
#
rm(list=ls())       # clear objects in memory#
library(ri)       # load the RI package#
library(foreign)    # package allows R to read Stata datasets#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
# Data are from Arceneaux, Kevin. 2005. “Using Cluster Randomized Field Experiments to Study Voting Behavior.” The Annals of the American Academy of Political and Social Science 601: 169-79.#
#
# read in data using Stata file - loaded directly from website.#
kansas <- read.dta("http://hdl.handle.net/10079/r2280ss")#
#
# Alternatively, you can instead read in data using .csv file#
# kansas <- read.csv(file="http://hdl.handle.net/10079/dr7sr5q",head=TRUE,sep=",")#
#
#redefine variables#
Z <-  kansas$treatmen#
Y <- kansas$vote03#
clust <- kansas$unit#
#
covs <- as.matrix(kansas[,2:21])  # covariates are past voter turnout if you care to perform a randomization check#
#
probs <- genprobexact(Z,clustvar=clust)  # subjects are clustered by precinct#
#
numiter <- 1000  # actual number of randomizations in this case is 40116600#
#
perms <- genperms(Z,maxiter=numiter,clustvar=clust)    # clustered assignment#
numiter <- ncol(perms)  # reset numiter so that it is no larger than the maximum number of possible randomizations#
#
## show the number of observations#
nrow(as.matrix(Y))#
#
## sort the data by cluster size in order to see the range#
sort(as.matrix((table(clust))))
Part (b) #
# Estimate the ATE using (possibly biased) difference-in-means#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                                  # estimated ATE#
sum(distout <= ate)                  # one-tailed comparison#
sum(abs(distout) >= abs(ate))        # two-tailed comparison#
dispdist(distout,ate)                #
#
# estimate the confidence interval assuming that the true ATE=ate#
Ys <- genouts(Y,Z,ate=ate)#
distout <- gendist(Ys,perms,prob=probs)#
dispdist(distout,ate)      #
#
# compare the p-value from RI to regression outcome assuming naive standard errors#
summary(lm(Y ~ Z)
summary(lm(Y ~ Z)
summary(lm(Y ~ Z))
regression with robust cluster standard errors#
#
# R requires a hand-made function#
cl   <- function(dat,fm, cluster){#
  require(sandwich, quietly = TRUE)#
  require(lmtest, quietly = TRUE)#
  M <- length(unique(cluster))#
  N <- length(cluster)#
  K <- fm$rank#
  dfc <- (M/(M-1))*((N-1)/(N-K))#
  uj  <- apply(estfun(fm),2, function(x) tapply(x, cluster, sum));#
  vcovCL <- dfc*sandwich(fm, meat=crossprod(uj)/N)#
  coeftest(fm, vcovCL) }#
#
cluster.frame <- data.frame(Y,Z,clust)#
naive.fit <- lm(Y ~ Z, data=cluster.frame)#
cl(cluster.frame, naive.fit, cluster=clust)
This is the script we wrote together in section on Wednesday 2/5/14#
# it does 2 things using randomization inference: #
#   1. test the sharp null of no effect#
#   2. conduct a balance test using simulated f statistics#
# this is all done by hand to make the connection with the RI package.#
rm(list=ls())  ## clear objects in memory#
#
## generate fake data#
#
N <- 100                        # Sample size#
Y <- rnorm(N)                   # outcome#
Z <- rbinom(N, size=1,prob=.4)  # Simple Random Assignment#
X <- rpois(N, lambda=4)         # Covariate for balance test (the f-test)#
#
## gather "observed" statistics#
ate.obs <- mean(Y[Z==1]) - mean(Y[Z==0])#
fit <- lm(Z ~ X)#
f.obs <- summary(fit)$fstatistic[1]#
#
### Generate full schedule of potential outcomes#
# Super easy under sharp null of no effect#
Y0 <- Y#
Y1 <- Y#
#
sims <- 1000#
#
Z.block <- matrix(NA, nrow=N, ncol=sims) ## Empty matrix for simulated random assignments#
#
## loop to make "sims" number of random assignments#
for(i in 1:sims){#
Z.block[,i] <- rbinom(N, size=1,prob=.4)#
}#
#
## empty vectors to collect simulated statistics#
ests <- rep(NA, sims)  #
f.sims <- rep(NA, sims)#
#
for(k in 1:sims){#
  Z.sims <- Z.block[,k]           # grab the kth random assignment#
  Y.sims <- Y1*Z.sims + (1-Z.sims)*Y0   # switching equation to "reveal" potential outcomes#
  est.sims <- mean(Y.sims[Z.sims ==1]) - mean(Y.sims[Z.sims==0])   #estimate simulated ate#
  ests[k] <- est.sims # store it#
  fit.sims <- lm(Z.sims ~ X)  # regression of assignment on the covariate#
  f.sims[k] <- summary(fit.sims)$fstatistic[1]  # grab simulated fstat, store it.#
}#
### Pictures#
hist(ests)#
abline(v=ate.obs, col="red", lty=3, lwd=3)#
#
hist(f.sims)#
abline(v=f.obs, col="red", lty=3, lwd=3)#
#
## P-values#
mean(ests >=ate.obs)#
mean(f.sims >= f.obs)
Kinds of Random Assignment#
#
complete.ra.multiple.arms <- function(n, num_arms, groupsizes=NULL){#
  indices <- 1:n#
  assign <- rep(NA, n)#
  if (is.null(groupsizes)){#
    for (i in 1:num_arms){#
      chosen  <- sample(indices, (n/num_arms))#
      assign[chosen] <- paste0("T",i)#
      indices <- indices[!indices %in% chosen]#
    }#
    return(assign)#
  }#
  for (i in 1:length(groupsizes)){#
    chosen <- sample(indices, groupsizes[i])#
    assign[chosen] <- paste0("T",i)#
    indices <- indices[!indices %in% chosen]#
  } #
  return(assign)#
}#
#
block.ra <- function(blockvar, block.m){#
  blocks <- sort(unique(blockvar))#
  assign <- rep(NA, length(blockvar))#
  for(i in 1:length(blocks)){#
    N.block <- sum(blockvar==blocks[i])#
    Z[blockvar==blocks[i]] <- ifelse(1:N.block %in% sample(1:N.block, block.m[i]),1,0)#
  }#
  return(assign)#
}#
#
complete.ra <- function(N,m){#
  assign <- ifelse(1:N %in% sample(1:N,m),1,0)#
  return(assign)#
}#
#
simple.ra <- function(N, prob){#
  assign <- rbinom(n=N,size=1,prob=prob)#
  return(assign)#
}
simple.ra(10,.5)
complete.ra(10,4)
complete.ra.multiple.arms(10,2)
complete.ra.multiple.arms(10,2,groupsizes=5)
Kinds of Random Assignment#
#
complete.ra.multiple.arms <- function(n, num_arms, groupsizes=NULL){#
  indices <- 1:n#
  assign <- rep(NA, n)#
  if (is.null(groupsizes)){#
    for (i in 1:num_arms){#
      chosen  <- sample(indices, (n/num_arms))#
      assign[chosen] <- paste0("T",i)#
      indices <- indices[!indices %in% chosen]#
    }#
    return(assign)#
  }#
  for (i in 1:length(groupsizes)){#
    chosen <- sample(indices, groupsizes[i])#
    assign[chosen] <- paste0("T",i)#
    indices <- indices[!indices %in% chosen]#
  } #
  return(assign)#
}
block <- c(1,1,2,2,2)
block
block.ra(block,1)
block.ra(block,5)
block.ra(block,4)
block.ra(block,2)
rm(list=ls())#
library(dplyr)
# Exercise 3.8 (Legislative terms natural experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 3/")#
#
library(foreign)    # package allows R to read Stata datasets#
#
# Data are from Titiunik, Rocío. 2010. “Drawing Your Senator from a Jar: Term Length and Legislative Behavior.” University of Michigan Working Paper. #
#
titiunik <- read.dta("Titiunik data for Exercises to Chapter 3.dta")#
#
Z <-  titiunik$term2year       # treatment is 2 year rather than 4 year term#
Y <- titiunik$bills_introduced#
block <- titiunik$texas0_arkansas1   # randomization occurs within each state#
#
probs <- genprobexact(Z,blockvar=block)   # blocking is assumed when generating probability of treatment#
table(probs)#
#
ate <- estate(Y,Z,prob=probs)      # estimate the ATE#
#
perms <- genperms(Z,maxiter=10000,blockvar=block)   # set the number of simulated random assignments#
Ys <- genouts(Y,Z,ate=0)    # create potential outcomes under the sharp null of no effect for any unit#
#
distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
#
ate                             # estimated ATE#
mean(abs(distout) >= abs(ate))  # two-tailed comparison used to calculate p-value#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
# illustration that naive estimation of the ATE is misleading#
mean(Y[Z==1 & block==0])#
mean(Y[Z==0 & block==0])#
#
mean(Y[Z==1 & block==1])#
mean(Y[Z==0 & block==1])#
#
mean(block==0)#
mean(block==1)#
#
# put the formula together to create a block-by-block weighted avg#
mean(block==0)*(mean(Y[Z==1 & block==0])-mean(Y[Z==0 & block==0])) + mean(block==1)*(mean(Y[Z==1 & block==1])-mean(Y[Z==0 & block==1]))#
#
# generate IPW weights#
ipw <- (1-block)*(Z/mean(Z[block==0]) + (1-Z)/(1-mean(Z[block==0]))) + #
       (block)  *(Z/mean(Z[block==1]) + (1-Z)/(1-mean(Z[block==1])))#
#
# weighted regression with IPW weights#
summary(lm(Y ~ Z , weights = ipw))#
#
# naive estimate (ignoresd blocking with unequal probabilities of treatment)#
mean(Y[Z==1])-mean(Y[Z==0])#
# LSDV estimate#
summary(lm(Y ~ Z + block))#
#
# note what happens when one includes both weights and block dummy#
# same estimated ATE, different standard errors (because blocks predict Y)#
# notice that the p-value here matches RI quite closely#
summary(lm(Y ~ Z + block, weights = ipw))
