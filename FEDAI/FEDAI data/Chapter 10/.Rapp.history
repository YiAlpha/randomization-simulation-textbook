storemean
100/2.5
villages <- cbind(rep(0,25),rep(1,75))#
#
#
numiter <- 10#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
uppwe <- round(numiter - numiter/40,0)
villages <- cbind(rep(0,25),rep(1,75))#
#
#
numiter <- 10#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
lower#
upper
villages <- cbind(rep(0,25),rep(1,75))#
#
#
numiter <- 100#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
lower#
upper
villages <- cbind(rep(0,25),rep(1,75))#
#
#
numiter <- 1000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
lower#
upper
villages <- cbind(rep(0,25),rep(1,75))#
#
#
numiter <- 1000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
lower[1:20]
storemean[1:20]
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,50,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages/2),replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
length[storemean]
length(storemean)
lower
upper
length(villages)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages/2),replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- cbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
length(villages)
mean(villages)
cbind(rep(0,25),rep(1,75))
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- rbind(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
rbind(rep(0,25),rep(1,75))
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
storemean[lower]#
storemean[upper]
villages
length(villages)
lower
upper
length(storemean)
mean(random_draw)
??"decimals"
?ndec
??"digits"
?digit
??digit
??digits
?display
??display
??environment
sys.getenv
??environment
help base
?base
library(help="base")
Sys.localeconv
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result*100#
upper_result*100
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result*10#
upper_result*10
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(random_draw)
mean(control_draw)
length(villages)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages/2)))#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
length(ethnicity)
corr(ethnicity,treatment)
r(ethnicity,treatment)
correlation(ethnicity,treatment)
?correlation
??correlation
mean(treatment)
length(treatment)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(ethnicity,treatment)#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
corr(ethnicity,treatment)
??correlation
corr(ethnicity,treatment)
length(ethnicity)
length(treatment)
?corr
corr(cbind(ethnicity,treatment))
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr)
mean(ethnicity)
mean(treatment)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr)
cbind(ethnicity,treatment)
mean(ethnicity)
control_draw
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
seed <- 1337#
rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        control_draw <- abs(1-random_draw)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
3+3
?sort
??sort
random_draw
table(random_draw)
nrows(table(random_draw))
nrow(table(random_draw))
mean(random_draw)
control_draw <- c(rep(1,75-table(random_draw)[2,2]))
test=table(random_draw)
test[2,2]
test[2,1]
test(2,1)
length(test)
test[1]
test[1,1]
test
test[2]
length(test[2])
length(test[1])
nrow(test)
ncol(test)
test[1]*test[2]
test2 <- cbind(rand_sort,villages)
test2
sort(test2)
as.matrix(test)
testmat <- as.matrix(test)
testmat
testmat[1,1]
dims(testmat)
dim(testmat)
test
testmat
nrow(testmat)
ncol(testmat)
testmat[1,1]
testmat[2,1]
ethnicity[treatment ==1]
ethnicity[treatment ==0]
random_draw
mean(random_draw)
table(random_draw)
mean(villages)
mean(villages)*length(villages)
mean(random_draw)*length(villages)/2
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones)#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))å#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
control_ones
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr[iter] <- corr(cbind(ethnicity,treatment))#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^0.5#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
mean(storecorr2)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 1000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^0.5#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
storecorr
storecorr2
ethnicity
mean(random_draw)
mean(control_draw)
length(random_draw)
length(control_draw)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 1000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 1000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result#
#
mean(storecorr2)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
lower_result#
upper_result#
#
mean(storecorr2)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
# number of villages in simulation#
length(villages)#
lower_result#
upper_result#
#
mean(storecorr2)
hist(storecorr2)
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
 villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
# seed <- 1337#
# rand_sort=rnorm(length(villages))#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
# number of villages in simulation#
length(villages)#
lower_result#
upper_result#
#
storecorr2 <- sort(storecorr2)#
mean(storecorr2)
storecorr2[2500]
storecorr2[97500]
storecorr2[99990]
storecorr2[99999]
# program to illustrate how sample size improves balance#
# (and, by implication) the gains from blocking diminish)#
#
# case 1: 75 Hindu villages and 25 non-Hindu villages#
villages <- c(rep(0,25),rep(1,75))#
#
# case 2: 750 Hindu villages and 250 non-Hindu villages#
# villages <- rep(villages,10)#
 treatment <- c(rep(1,length(villages)/2),rep(0,length(villages)/2))#
#
#
numiter <- 100000#
#
storecorr2 <- storemean <- rep(NA,numiter)#
#
for (iter in 1:numiter)#
                       {#
						random_draw <- sample(villages,length(villages)/2,replace=FALSE)#
                        storemean[iter] <- mean(random_draw)#
                        #
                        # generate the control group, which is the rest of the villages#
   control_ones  <- rep(1,mean(villages)*length(villages) - mean(random_draw)*length(villages)/2)#
   control_zeros <- rep(0,length(random_draw) - length(control_ones))#
                        control_draw <- c(control_ones,control_zeros)#
                        ethnicity <- c(random_draw,control_draw)#
                        storecorr2[iter] <- (corr(cbind(ethnicity,treatment)))^2#
                        }#
#
storemean <- sort(storemean)#
# find 2.5th and 97.5th percentiles#
#
lower <- round(numiter/40,0)#
upper <- round(numiter - numiter/40,0)#
#
lower_result <- storemean[lower]#
upper_result <- storemean[upper]#
#
# number of replications#
numiter#
# number of villages in simulation#
length(villages)#
lower_result#
upper_result#
#
storecorr2 <- sort(storecorr2)#
mean(storecorr2)
storecorr2[97500]
storecorr2[99900]
hist(storecorr2)
?logit
??logit
?logit
?"logistic regression"
??"logistic regression"
??"robust cluster"
??"cluster"
Z <- dshort_term=="4 years"
ibrary(foreign)#
#
#
Term <- read.dta("Chapter 13_Titiunik (2010) Dataset.dta")#
#
attach(Term)#
#
Z_alpha <- dshort_term
set.seed(1234567)#
#
library(ri)#
library(foreign)#
#
hough <- read.dta("/Users/donaldgreen/Dropbox/Field Experimentation Book/Datasets for Website/Chapter 8_Leslie Hough self-experiment data.dta")#
#
# Part (b)#
#
Y <- hough$tetris#
Z <- hough$run#
#
N <- length(Z)#
#
Zlag <- c(NA,Z[2:N-1]) # exclude day 1 from analysis#
Ylag <- c(NA,Y[2:N-1])#
#
randfun <- function() rbinom(N,1,.5)#
#
numiter <- 10000#
perms <- genperms.custom(numiter=numiter,randfun=randfun)#
#
test1 <- lm(Y~Z)$coefficients["Z"]#
test2 <- summary(lm(Y~Z+Zlag))$fstatistic[1]#
test3 <- lm(Ylag~Z)$coefficients["Z"]#
test4 <- lm(hough$energy~Z)$coefficients["Z"]#
test5 <- lm(hough$gre~Z)$coefficients["Z"]#
#
testdist1 <- testdist2 <- testdist3 <- testdist4 <- testdist5 <- rep(NA,numiter)#
#
for (i in 1:numiter) {#
	#
	Zri <- perms[,i]#
	Zlagri <- c(NA,Zri[2:N-1]) # exclude day 1 from analysis#
#
testdist1[i] <- lm(Y~Zri)$coefficients["Zri"]#
testdist2[i] <- summary(lm(Y~Zri+Zlagri))$fstatistic[1]#
testdist3[i] <- lm(Ylag~Zri)$coefficients["Zri"]#
testdist4[i] <- lm(hough$energy~Zri)$coefficients["Zri"]#
testdist5[i] <- lm(hough$gre~Zri)$coefficients["Zri"]#
	#
	}#
	#
mean(testdist1 >= test1)#
mean(testdist2 >= test2)#
mean(abs(testdist3) >= abs(test3))#
mean(testdist4 >= test4)#
mean(testdist5 >= test5)
library(ri)#
#
set.seed(1)#
#
Y1 <- c(5,15,12,19,17,18,24,11,16,25,18,21,17,24,27,26,30,37,43,39,36,27,33,37,48,39,42,37,53,50,51,43,55,49,48,52,59,52,55,63)#
Y0 <- c(5,5,6,9,10,11,12,13,14,19,20,20,20,21,24,25,27,27,30,32,32,32,32,35,35,37,38,38,41,42,43,44,45,47,48,51,52,52,57,62)#
X <- c(6,8,5,13,9,15,16,17,19,23,28,28,9,16,23,15,23,33,42,31,29,28,35,28,41,37,32,37,36,44,48,43,55,53,51,43,57,51,49,55)#
#
mean(Y1-Y0)#
#
### DGP.#
#
Z <- c(0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,0,1,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,0,1,1)#
Y <- Y0*(1-Z) + Y1*(Z)#
N <- length(Z)#
#
# Part (a)#
#
lm(Y~Z)#
mean(Y[Z==1])-mean(Y[Z==0])#
#
# Part (b)#
#
lm(Y~X,subset=Z==1)#
lm(Y~X,subset=Z==0)#
#
# Part (c)#
#
lm(Y~Z+X)#
#
# Part (d)#
#
perms <- genperms(Z,maxiter=100000)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
#
dispdist(distout,ate)#
#
# Part (e)#
#
ateX <- estate(Y,Z,X,prob=probs)#
#
distoutX <- gendist(Ys,perms,X,prob=probs)#
#
ateX#
#
dispdist(distoutX,ateX)#
#
# Part (f)#
#
Ys2 <- genouts(Y,Z,ate=ate)#
#
distout2 <- gendist(Ys2,perms,prob=probs)#
#
ate#
#
dispdist(distout2,ate)#
#
# Part (g)#
#
YsX <- genouts(Y,Z,ate=ateX)#
#
distoutX2 <- gendist(YsX,perms,X,prob=probs)#
#
ateX#
#
dispdist(distoutX2,ateX)
# Exercise 3.8 (Legislative terms natural experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 3/")#
#
library(foreign)    # package allows R to read Stata datasets#
#
# Data are from Titiunik, Rocío. 2010. “Drawing Your Senator from a Jar: Term Length and Legislative Behavior.” University of Michigan Working Paper. #
#
titiunik <- read.dta("Titiunik data for Exercises to Chapter 3.dta")#
#
Z <-  titiunik$term2year       # treatment is 2 year rather than 4 year term#
Y <- titiunik$bills_introduced#
block <- titiunik$texas0_arkansas1   # randomization occurs within each state#
#
probs <- genprobexact(Z,blockvar=block)   # blocking is assumed when generating probability of treatment#
#
ate <- estate(Y,Z,prob=probs)      # estimate the ATE#
#
perms <- genperms(Z,maxiter=10000,blockvar=block)   # set the number of simulated random assignments#
#
#
Ys <- genouts(Y,Z,ate=0)    # create potential outcomes under the sharp null of no effect for any unit#
#
distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
#
ate                             # estimated ATE#
sum(abs(distout) >= abs(ate))   # two-tailed comparison used to calculate p-value#
#
#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 10000   # set the number of simulated random assignments#
#
perms <- genperms(Z,maxiter=numiter)  #
#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]  # observed F statistic#
#
Fstatstore <- rep(NA,numiter)    # initialize vector of simulated F statistics#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]#
	}#
#
mean(Fstatstore >= Fstat)    # calculate p-value#
#
#
#
# look at ATE using raw scores as outcomes (ignoring pretest)#
#
perms <- genperms(Z,maxiter=100000)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                         # estimated ATE#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)  # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)   # display p-values (ignore these), 95% confidence interval, standard error under the assumed DGP, and graph the sampling distribution under the assumption that the Y1-Y0=estimated(ATE) for all units#
#
#
#
# look at ATE using improvement scores (posttest-pretest) as outcomes#
#
Y <- rush$improvement#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)#
Ys <- genouts(Y,Z,ate=0)   # the statement ate=0 imposes the assumption that the true treatment effect for every unit is 0#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)   # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
distout <- gendist(Ys,perms,prob=probs)#
ate#
dispdist(distout,ate)#
#
#
#
# for fun, look at effect on posttest controlling for covariate using regression#
Y <- rush$posttest#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ateX <- estate(Y,Z,X,prob=probs)  # this syntax controls for X#
Ys <- genouts(Y,Z,ate=0)#
distoutX <- gendist(Ys,perms,X,prob=probs)#
ateX#
dispdist(distoutX,ateX)
summary(lm(Y~Z+X))
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 50000   # set the number of simulated random assignments (if you set it to 48620 or higher, your results will be exact because that is the true number of possible random assignments)#
#
perms <- genperms(Z,maxiter=numiter)  #
#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]  # observed F statistic#
#
Fstatstore <- rep(NA,numiter)    # initialize vector of simulated F statistics#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]#
	}#
#
mean(Fstatstore >= Fstat)    # calculate p-value#
#
#
#
# look at ATE using raw scores as outcomes (ignoring pretest)#
#
perms <- genperms(Z,maxiter=100000)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                         # estimated ATE#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)  # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)   # display p-values (ignore these), 95% confidence interval, standard error under the assumed DGP, and graph the sampling distribution under the assumption that the Y1-Y0=estimated(ATE) for all units#
#
#
#
# look at ATE using improvement scores (posttest-pretest) as outcomes#
#
Y <- rush$improvement#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)#
Ys <- genouts(Y,Z,ate=0)   # the statement ate=0 imposes the assumption that the true treatment effect for every unit is 0#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)   # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
distout <- gendist(Ys,perms,prob=probs)#
ate#
dispdist(distout,ate)#
#
#
#
# for fun, look at effect on posttest controlling for covariate using regression and compare RI results to regression output#
Y <- rush$posttest#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ateX <- estate(Y,Z,X,prob=probs)  # this syntax controls for X#
Ys <- genouts(Y,Z,ate=0)#
distoutX <- gendist(Ys,perms,X,prob=probs)#
ateX#
dispdist(distoutX,ateX)#
#
summary(lm(Y~Z+X))  # the estimated ATE is identical, compare p-values and SEs
numiter
nrow(perms)
dim(perms)
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 500   # set the number of simulated random assignments (if you set it to 48620 or higher, your results will be exact because that is the true number of possible random assignments)#
#
perms <- genperms(Z,maxiter=numiter)  #
#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]  # observed F statistic#
#
Fstatstore <- rep(NA,numiter)    # initialize vector of simulated F statistics#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]#
	}#
#
mean(Fstatstore >= Fstat)    # calculate p-value#
#
#
#
# look at ATE using raw scores as outcomes (ignoring pretest)#
#
perms <- genperms(Z,maxiter=100000)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                         # estimated ATE#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)  # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)   # display p-values (ignore these), 95% confidence interval, standard error under the assumed DGP, and graph the sampling distribution under the assumption that the Y1-Y0=estimated(ATE) for all units#
#
#
#
# look at ATE using improvement scores (posttest-pretest) as outcomes#
#
Y <- rush$improvement#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)#
Ys <- genouts(Y,Z,ate=0)   # the statement ate=0 imposes the assumption that the true treatment effect for every unit is 0#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)   # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
distout <- gendist(Ys,perms,prob=probs)#
ate#
dispdist(distout,ate)#
#
#
#
# for fun, look at effect on posttest controlling for covariate using regression and compare RI results to regression output#
Y <- rush$posttest#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ateX <- estate(Y,Z,X,prob=probs)  # this syntax controls for X#
Ys <- genouts(Y,Z,ate=0)#
distoutX <- gendist(Ys,perms,X,prob=probs)#
ateX#
dispdist(distoutX,ateX)#
#
summary(lm(Y~Z+X))  # the estimated ATE is identical, compare p-values and SEs
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 500   # set the number of simulated random assignments (if you set it to 48620 or higher, your results will be exact because that is the true number of possible random assignments)#
#
perms <- genperms(Z,maxiter=numiter)  #
#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]  # observed F statistic#
#
Fstatstore <- rep(NA,numiter)    # initialize vector of simulated F statistics#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]#
	}#
#
mean(Fstatstore >= Fstat)    # calculate p-value#
#
#
#
# look at ATE using raw scores as outcomes (ignoring pretest)#
#
perms <- genperms(Z,maxiter=numiter)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                         # estimated ATE#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)  # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)   # display p-values (ignore these), 95% confidence interval, standard error under the assumed DGP, and graph the sampling distribution under the assumption that the Y1-Y0=estimated(ATE) for all units#
#
#
#
# look at ATE using improvement scores (posttest-pretest) as outcomes#
#
Y <- rush$improvement#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)#
Ys <- genouts(Y,Z,ate=0)   # the statement ate=0 imposes the assumption that the true treatment effect for every unit is 0#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)   # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
distout <- gendist(Ys,perms,prob=probs)#
ate#
dispdist(distout,ate)#
#
#
#
# for fun, look at effect on posttest controlling for covariate using regression and compare RI results to regression output#
Y <- rush$posttest#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ateX <- estate(Y,Z,X,prob=probs)  # this syntax controls for X#
Ys <- genouts(Y,Z,ate=0)#
distoutX <- gendist(Ys,perms,X,prob=probs)#
ateX#
dispdist(distoutX,ateX)#
#
summary(lm(Y~Z+X))  # the estimated ATE is identical, compare p-values and SEs
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 500   # set the number of simulated random assignments (if you set it to 48620 or higher, your results will be exact because that is the true number of possible random assignments)#
#
perms <- genperms(Z,maxiter=numiter)  #
#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]  # observed F statistic#
#
Fstatstore <- rep(NA,numiter)    # initialize vector of simulated F statistics#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]#
	}#
#
mean(Fstatstore >= Fstat)    # calculate p-value#
#
#
#
# look at ATE using raw scores as outcomes (ignoring pretest)#
#
perms <- genperms(Z,maxiter=numiter)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                         # estimated ATE#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)  # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)   # display p-values (ignore these), 95% confidence interval, standard error under the assumed DGP, and graph the sampling distribution under the assumption that the Y1-Y0=estimated(ATE) for all units#
#
#
#
# look at ATE using improvement scores (posttest-pretest) as outcomes#
#
Y <- rush$improvement#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)#
Ys <- genouts(Y,Z,ate=0)   # the statement ate=0 imposes the assumption that the true treatment effect for every unit is 0#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)   # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
distout <- gendist(Ys,perms,prob=probs)#
ate#
dispdist(distout,ate)#
#
#
#
# for fun, look at effect on posttest controlling for covariate using regression and compare RI results to regression output#
Y <- rush$posttest#
perms <- genperms(Z,maxiter=100000)#
probs <- genprobexact(Z)#
ateX <- estate(Y,Z,X,prob=probs)  # this syntax controls for X#
Ys <- genouts(Y,Z,ate=0)#
distoutX <- gendist(Ys,perms,X,prob=probs)#
ateX#
dispdist(distoutX,ateX)#
#
summary(lm(Y~Z+X))  # the estimated ATE is identical, compare p-values and SEs
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 500   # set the number of simulated random assignments (if you set it to 48620 or higher, your results will be exact because that is the true number of possible random assignments)#
#
perms <- genperms(Z,maxiter=numiter)  #
#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]  # observed F statistic#
#
Fstatstore <- rep(NA,numiter)    # initialize vector of simulated F statistics#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]#
	}#
#
mean(Fstatstore >= Fstat)    # calculate p-value#
#
#
#
# look at ATE using raw scores as outcomes (ignoring pretest)#
#
perms <- genperms(Z,maxiter=numiter)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                         # estimated ATE#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)  # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)   # display p-values (ignore these), 95% confidence interval, standard error under the assumed DGP, and graph the sampling distribution under the assumption that the Y1-Y0=estimated(ATE) for all units#
#
#
#
# look at ATE using improvement scores (posttest-pretest) as outcomes#
#
Y <- rush$improvement#
perms <- genperms(Z,maxiter=numiter)#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)#
Ys <- genouts(Y,Z,ate=0)   # the statement ate=0 imposes the assumption that the true treatment effect for every unit is 0#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)   # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
distout <- gendist(Ys,perms,prob=probs)#
ate#
dispdist(distout,ate)#
#
#
#
# for fun, look at effect on posttest controlling for covariate using regression and compare RI results to regression output#
Y <- rush$posttest#
perms <- genperms(Z,maxiter=numiter)#
probs <- genprobexact(Z)#
ateX <- estate(Y,Z,X,prob=probs)  # this syntax controls for X#
Ys <- genouts(Y,Z,ate=0)#
distoutX <- gendist(Ys,perms,X,prob=probs)#
ateX#
dispdist(distoutX,ateX)#
#
summary(lm(Y~Z+X))  # the estimated ATE is identical, compare p-values and SEs
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 50000   # set the number of simulated random assignments (if you set it to 48620 or higher, your results will be exact because that is the true number of possible random assignments)#
#
perms <- genperms(Z,maxiter=numiter)
dim(perms)
maxiter
numiter <- ncol(perms)
numiter
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Dan Gendelman (www.eshcolot.com), who studied the effects of a teaching curriculum on elementary students' performance at the puzzle called "Rush Hour"#
#
rush <- read.dta("RushHour data for exercise 4-2.dta")#
#
Z <- rush$treat       # treatment is thinking strategies curriculum#
Y <- rush$posttest    # number of puzzled solved during the testing session#
X <- rush$pretest#
#
# RI: randomization check, testing the effect of the covariate on the treatment#
#
covs <- as.matrix(rush$pretest)     # covariate is the pretest score#
#
probs <- genprobexact(Z)#
#
numiter <- 50000   # set the number of simulated random assignments (if you set it to 48620 or higher, your results will be exact because that is the true number of possible random assignments)#
#
perms <- genperms(Z,maxiter=numiter)  #
numiter <- ncol(perms)  # reset numiter so that it is no larger than the maximum number of possible randomizations#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]  # observed F statistic#
#
Fstatstore <- rep(NA,numiter)    # initialize vector of simulated F statistics#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]#
	}#
#
mean(Fstatstore >= Fstat)    # calculate p-value#
#
#
#
# look at ATE using raw scores as outcomes (ignoring pretest)#
#
perms <- genperms(Z,maxiter=numiter)#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                         # estimated ATE#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)  # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)   # display p-values (ignore these), 95% confidence interval, standard error under the assumed DGP, and graph the sampling distribution under the assumption that the Y1-Y0=estimated(ATE) for all units#
#
#
#
# look at ATE using improvement scores (posttest-pretest) as outcomes#
#
Y <- rush$improvement#
perms <- genperms(Z,maxiter=numiter)#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)#
Ys <- genouts(Y,Z,ate=0)   # the statement ate=0 imposes the assumption that the true treatment effect for every unit is 0#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
dispdist(distout,ate)#
#
#
# compute confidence intervals#
Ys <- genouts(Y,Z,ate=ate)   # the statement ate=ate imposes the assumption that the true treatment effect for every unit is equal to the estimated ATE#
distout <- gendist(Ys,perms,prob=probs)#
ate#
dispdist(distout,ate)#
#
#
#
# for fun, look at effect on posttest controlling for covariate using regression and compare RI results to regression output#
Y <- rush$posttest#
perms <- genperms(Z,maxiter=numiter)#
probs <- genprobexact(Z)#
ateX <- estate(Y,Z,X,prob=probs)  # this syntax controls for X#
Ys <- genouts(Y,Z,ate=0)#
distoutX <- gendist(Ys,perms,X,prob=probs)#
ateX#
dispdist(distoutX,ateX)#
#
summary(lm(Y~Z+X))  # the estimated ATE is identical, compare p-values and SEs
# Exercise 4.10 -- Kansas City Clustered Design#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Datasets for Website/")#
#
library(foreign)    # package allows R to read Stata datasets#
#
# Data are from Arceneaux, Kevin. 2005. “Using Cluster Randomized Field Experiments to Study Voting Behavior.” The Annals of the American Academy of Political and Social Science 601: 169-79.#
#
kansas <- read.dta("Chapter 4_Arceneaux (2005) Dataset.dta")#
#
Z <-  kansas$treatmen#
Y <- kansas$vote03#
clust <- kansas$unit#
#
covs <- as.matrix(kansas[,2:21])  # covariates are past voter turnout#
#
probs <- genprobexact(Z,clustvar=clust)  # subjects are clustered by precinct#
#
numiter <- 1000#
#
perms <- genperms(Z,maxiter=numiter,clustvar=clust)    # clustered assignment#
numiter <- ncol(perms)  # reset numiter so that it is no larger than the maximum number of possible randomizations
numiter
# Exercise 3.9 parts b and e#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 3/")#
#
library(foreign)    # package allows R to read Stata datasets#
#
# Data are from Camerer, Colin F. . 1998. “Can Asset Markets Be Manipulated? A Field Experiment with Racetrack Betting.” Journal of Political Economy 106(3): 457-482.#
#
camerer <- read.dta("Camerer data for Chapter 3 exercises.dta")#
colnames(camerer)#
#
Z <-  camerer$treatment#
Y <- camerer$experimentbets#
block <- camerer$pair        # indicates how the subjects are blocked      #
#
# RI for the effect of the covariate on the treatment#
#
covs <- as.matrix(camerer$preexperimentbets)     # reads in covariates#
#
probs <- genprobexact(Z,blockvar=block)#
#
numiter <- 10000#
#
perms <- genperms(Z,maxiter=numiter,blockvar=block)#
numiter <- ncol(perms)  # reset numiter so that it is no larger than the maximum number of possible randomizations#
#
## Use F-test to assess the null hypothesis that the covariates predict random assignment (Z) no better than would be expected by chance#
#
Fstat <- summary(lm(Z~covs))$fstatistic[1]   # F-statistic from actual data#
#
Fstatstore <- rep(NA,numiter)#
#
for (i in 1:numiter) {#
	Fstatstore[i] <- summary(lm(perms[,i]~covs))$fstatistic[1]  # F-statistic under the null of random assignment of Z#
	}#
#
mean(Fstatstore >= Fstat)                     # p-value
library(ri)#
#
set.seed(1)#
#
Y1 <- c(5,15,12,19,17,18,24,11,16,25,18,21,17,24,27,26,30,37,43,39,36,27,33,37,48,39,42,37,53,50,51,43,55,49,48,52,59,52,55,63)#
Y0 <- c(5,5,6,9,10,11,12,13,14,19,20,20,20,21,24,25,27,27,30,32,32,32,32,35,35,37,38,38,41,42,43,44,45,47,48,51,52,52,57,62)#
X <- c(6,8,5,13,9,15,16,17,19,23,28,28,9,16,23,15,23,33,42,31,29,28,35,28,41,37,32,37,36,44,48,43,55,53,51,43,57,51,49,55)
Y1
cov(Y1,Y0,X)
cov(Y1,Y0)
cov(cbind(Y1,Y0,X))
# Exercise 4.2 (Game-playing experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Table 4.1#
#
teach <- read.dta("Teachers data for Table 4-1.dta")#
colnames(teach)
Z  <- teach$D       # treatment is thinking strategies curriculum#
Y1 <- teach$y1#
Y0 <- teach$y0#
X <- teach$x
cov(cbind(Z,Y1,Y0,X))
Y1 <- c(5,15,12,19,17,18,24,11,16,25,18,21,17,24,27,26,30,37,43,39,36,27,33,37,48,39,42,37,53,50,51,43,55,49,48,52,59,52,55,63)#
Y0 <- c(5,5,6,9,10,11,12,13,14,19,20,20,20,21,24,25,27,27,30,32,32,32,32,35,35,37,38,38,41,42,43,44,45,47,48,51,52,52,57,62)#
X <- c(6,8,5,13,9,15,16,17,19,23,28,28,9,16,23,15,23,33,42,31,29,28,35,28,41,37,32,37,36,44,48,43,55,53,51,43,57,51,49,55)#
#
mean(Y1-Y0)#
#
### DGP.#
#
Z <- c(0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,0,1,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,0,1,1)
cov(cbind(Z,Y1,Y0,X))
# Exercise 4.4 (teachers experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Table 4.1#
#
teach <- read.dta("Teachers data for Table 4-1.dta")#
colnames(teach)#
#
Z  <- teach$D       # treatment#
Y1 <- teach$y1      # treated potential outcome#
Y0 <- teach$y0      # untreated potential outcome#
X <- teach$x        # pre-test#
#
Y <- Y0*(1-Z) + Y1*(Z)    # observed outcomem given random assignment#
N <- length(Z)#
#
# Part (a)#
#
lm(Y~Z)#
mean(Y[Z==1])-mean(Y[Z==0])
lm(Y~X,subset=Z==1)#
lm(Y~X,subset=Z==0)
lm(Y~Z+X)
lm((Y-X)~Z)
Ydiff <- Y-X#
lm(Ydiff~Z)
Ydiff <- Y-X#
summary(lm(Ydiff~Z))
Ydiff <- Y-X#
summary(lm(Y~Z))
ateX <- estate(Y,Z,X,prob=probs)   # estimate the ATE using covariate adjustment
perms <- genperms(Z,maxiter=10000)  # simulate possible random allocations#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)       # estimate the ATE#
Ys <- genouts(Y,Z,ate=0)            # calculate potential outcomes under sharp null of 0 effect for all units#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate                                 # report the ATE#
dispdist(distout,ate)               # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
#
# Part (e)#
#
ateX <- estate(Y,Z,X,prob=probs)   # estimate the ATE using covariate adjustment
Ys <- genouts(Y,Z,ate=0)
# Exercise 4.4 (teachers experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Table 4.1#
#
teach <- read.dta("Teachers data for Table 4-1.dta")#
colnames(teach)#
#
Z  <- teach$D       # treatment#
Y1 <- teach$y1      # treated potential outcome#
Y0 <- teach$y0      # untreated potential outcome#
X <- teach$x        # pre-test#
#
Y <- Y0*(1-Z) + Y1*(Z)    # observed outcomem given random assignment#
N <- length(Z)#
#
# Part (a)#
#
# two equivalent ways to estimate the ATE: regression and difference-in-means#
summary(lm(Y~Z))#
mean(Y[Z==1])-mean(Y[Z==0])#
#
# Part (b)#
#
# a diagnostic test to assess whether rescaling the outcome as a change rather than a level is likely to improve the precision with which the ATE is estimated (see equation 4.6)#
lm(Y~X,subset=Z==1)#
lm(Y~X,subset=Z==0)#
#
Ydiff <- Y-X#
summary(lm(Ydiff~Z))#
#
# Part (c)#
#
summary(lm(Y~Z+X))   # estimate the ATE using covariate adjustment#
#
# Part (d)#
#
perms <- genperms(Z,maxiter=10000)  # simulate possible random allocations#
probs <- genprobexact(Z)#
ate <- estate(Y,Z,prob=probs)       # estimate the ATE#
Ys <- genouts(Y,Z,ate=0)            # calculate potential outcomes under sharp null of 0 effect for all units#
#
distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
#
ate                                 # report the estimated ATE#
dispdist(distout,ate)               # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
# Part (e)#
#
ateX <- estate(Y,Z,X,prob=probs)   # estimate the ATE using covariate adjustment#
distoutX <- gendist(Ys,perms,X,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
ateX                       # report the estimated ATE#
dispdist(distoutX,ateX)    #
#
#
# Part (f)#
#
Ys2 <- genouts(Y,Z,ate=ate)   # generate potential outcomes assuming that the true treatment effect for every unit is equal to the estimated ATE#
distout2 <- gendist(Ys2,perms,prob=probs)#
#
ate#
dispdist(distout2,ate)        # display the sampling distribution under the assumed treatment effect in order to obtain the confidence interval#
#
# Part (g)#
#
YsX <- genouts(Y,Z,ate=ateX)  # generate potential outcomes assuming that the true treatment effect for every unit is equal to the estimated ATE under covariate adjustment#
distoutX2 <- gendist(YsX,perms,X,prob=probs)#
#
ateX#
dispdist(distoutX2,ateX)
# Exercise 4.5 (restricted randomization)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Table 4.1#
#
teach <- read.dta("Teachers data for Table 4-1.dta")#
colnames(teach)#
#
Z  <- teach$D       # treatment#
Y1 <- teach$y1      # treated potential outcome#
Y0 <- teach$y0      # untreated potential outcome#
X <- teach$x        # pre-test#
#
Y <- Y0*(1-Z) + Y1*(Z)    # observed outcomem given random assignment#
N <- length(Z)#
#
# Part (a)#
# set up a restricted randomization whereby random allocations (Zri) are discarded if they generate an F-statistic whose p-value is smaller than 0.05 when Zri is regressed on the covariate X#
#
randfun <- function() {#
	teststat <- -1#
	while (teststat < 0.05) {#
		Zri <- sample(Z)#
		teststat <- summary(lm(Zri~X))$coefficients[2,4]   # extract the p-value from the F-test#
	}#
	return(Zri)#
}#
#
perms <- genperms.custom(numiter=10000,randfun=randfun)    # notice the use of the restricted randomization function in the generation of simulated random allocations#
#
probs <- genprob(perms)           # important: restricted randomization can sometimes generate unequal probabilities of assignment, so it's important to generate the probs and use inverse probability weights when estimating the ATE#
#
ate <- estate(Y,Z,prob=probs)    #
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
#
dispdist(distout,ate)#
#
# Part (b)#
# Compare the estimates and sampling distribution when covariate adjustment is used to estimate the ATE#
#
ateX <- estate(Y,Z,X,prob=probs)#
#
distoutX <- gendist(Ys,perms,X,prob=probs)#
#
ateX#
#
dispdist(distoutX,ateX)
# Exercise 4.4 (teachers experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 4/")#
#
#  Data are from Table 4.1#
#
teach <- read.dta("Teachers data for Table 4-1.dta")#
colnames(teach)#
#
Z  <- teach$D       # treatment#
Y1 <- teach$y1      # treated potential outcome#
Y0 <- teach$y0      # untreated potential outcome#
X <- teach$x        # pre-test#
#
Y <- Y0*(1-Z) + Y1*(Z)    # observed outcomem given random assignment#
N <- length(Z)#
#
# Part (a)#
#
# two equivalent ways to estimate the ATE: regression and difference-in-means#
summary(lm(Y~Z))#
mean(Y[Z==1])-mean(Y[Z==0])#
#
# Part (b)#
#
# a diagnostic test to assess whether rescaling the outcome as a change rather than a level is likely to improve the precision with which the ATE is estimated (see equation 4.6)#
lm(Y~X,subset=Z==1)#
lm(Y~X,subset=Z==0)#
#
Ydiff <- Y-X#
summary(lm(Ydiff~Z))
library(ri)#
#
set.seed(1234567)#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 6")#
#
library(foreign)#
#
hyde <- read.dta("Chapter 6_Hyde (2010) Dataset.dta")#
#
Z <- as.integer(hyde$Sample) -1#
Y <- hyde$invalidballots#
#
probs <- genprobexact(Z)#
#
ate <- estate(Y,Z,prob=probs)#
#
perms <- genperms(Z,maxiter=10000)#
#
Ys <- genouts(Y,Z,ate=0)#
#
distout <- gendist(Ys,perms,prob=probs)#
#
ate#
sum(distout >= ate)#
sum(abs(distout) >= abs(ate))#
#
dispdist(distout,ate)
# Exercise 6.10 (Election monitoring experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
# Data are from Hyde, Susan. 2010. “Experimenting in Democracy Promotion: International Observers and the 2004 Presidential Elections in Indonesia.” Perspectives on Politics 8:511-27.#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 6")#
#
hyde <- read.dta("Chapter 6_Hyde (2010) Dataset.dta")#
#
Z <- as.integer(hyde$Sample) -1   # monitoring treatment#
Y <- hyde$invalidballots#
#
probs <- genprobexact(Z)          # generate probability of treatment assignment#
ate <- estate(Y,Z,prob=probs)     # estimate the ITT (ATE of assignment)#
#
perms <- genperms(Z,maxiter=10000)  # set the number of simulated random assignments#
#
Ys <- genouts(Y,Z,ate=0)       # create potential outcomes under the sharp null of no effect for any unit#
#
distout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
#
ate                             # report the estimated ITT (ATE of assignment)#
sum(distout >= ate)#
sum(abs(distout) >= abs(ate))#
#
dispdist(distout,ate)       # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null
perms
dim(perms)
dim(perms)[2]
# Exercise 10.4 (Indian reservations for women natural experiment)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
#
setwd("/Users/donaldgreen/Dropbox/Field Experimentation Book/Final Code for Vignettes and Problems/Chapter 10")#
#
# Data are from Bhavnani, Rikhil R. 2009. “Do Electoral Quotas Work after They Are Withdrawn? Evidence from a Natural Experiment in India.” American Political Science Review 103: 23-35.#
#
bhav <- na.omit(read.dta("Chapter 10_Bhavnani (2009) Dataset.dta"))#
colnames(bhav)#
Z <- as.integer(bhav$controltreat) - 1     # treatment: reservations for women candidates#
Y <- bhav$turnout                          # an intermediate outcome: turnout#
#
#
probs <- genprobexact(Z)                # generate probability of treatment #
#
ate <- estate(Y,Z,prob=probs)           # estimate the ATE#
#
numiter <- 1000#
perms <- genperms(Z,maxiter=numiter)#
#
Ys <- genouts(Y,Z,ate=0)       # create potential outcomes under the sharp null of no effect for any unit#
#
distout <- gendist(Ys,perms,prob=probs)    # generate the sampling distribution  based on the schedule of potential outcomes implied by the null hypothesis#
#
ate#
sum(distout >= ate)#
sum(abs(distout) >= abs(ate))#
#
dispdist(distout,ate)      # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null#
#
#
# next, test for unequal variances using RI#
#
testvar <- var(Y[Z==1]) - var(Y[Z==0])#
#
varlist <- rep(NA,numiter)#
#
for (i in 1:dim(perms)[2]) {	#
	Zri <- perms[,i]#
	varlist[i] <- var(Y[Zri==1]) - var(Y[Zri==0])#
}#
#
mean(varlist >= testvar)             # p-value for one-tailed comparison#
mean(abs(varlist) >= abs(testvar))   # p-value for testing unequal variances
colnames(bhav)
testvar <- var(Y[Z==1]) - var(Y[Z==0])
testvar
var(Y[Z==1])
var(Y[Z==0])
plot(Y,Z)
plot(Z,Y)
hist(Y|Z)
histogram(Y|Z)
?hist
hist(y)
hist(Y)
hist(Y[Z==1])
set.seed(1234567)#
#
library(ri)#
library(foreign)#
#
hough <- read.dta("/Users/donaldgreen/Dropbox/Field Experimentation Book/Datasets for Website/Chapter 8_Leslie Hough self-experiment data.dta")#
#
# Part (b)#
#
Y <- hough$tetris#
Z <- hough$run#
#
N <- length(Z)#
#
Zlag <- c(NA,Z[2:N-1]) # exclude day 1 from analysis#
Ylag <- c(NA,Y[2:N-1])#
#
randfun <- function() rbinom(N,1,.5)#
#
numiter <- 10000#
perms <- genperms.custom(numiter=numiter,randfun=randfun)#
#
test1 <- lm(Y~Z)$coefficients["Z"]#
test2 <- summary(lm(Y~Z+Zlag))$fstatistic[1]#
test3 <- lm(Ylag~Z)$coefficients["Z"]#
test4 <- lm(hough$energy~Z)$coefficients["Z"]#
test5 <- lm(hough$gre~Z)$coefficients["Z"]#
#
testdist1 <- testdist2 <- testdist3 <- testdist4 <- testdist5 <- rep(NA,numiter)#
#
for (i in 1:numiter) {#
	#
	Zri <- perms[,i]#
	Zlagri <- c(NA,Zri[2:N-1]) # exclude day 1 from analysis#
#
testdist1[i] <- lm(Y~Zri)$coefficients["Zri"]#
testdist2[i] <- summary(lm(Y~Zri+Zlagri))$fstatistic[1]#
testdist3[i] <- lm(Ylag~Zri)$coefficients["Zri"]#
testdist4[i] <- lm(hough$energy~Zri)$coefficients["Zri"]#
testdist5[i] <- lm(hough$gre~Zri)$coefficients["Zri"]#
	#
	}#
	#
mean(testdist1 >= test1)#
mean(testdist2 >= test2)#
mean(abs(testdist3) >= abs(test3))#
mean(testdist4 >= test4)#
mean(testdist5 >= test5)
dim(testdist1)
length(testdist)
length(testdist1)
class(testdist1)
lm(Ylag~Zri)
length(Ylag)
length(Zri)
Ylag
?lm()
lm(Ylag~Zri)
lm(Ylag~Zri,na.action=omit)
?lm()
lm(Ylag~Zri,na.omit)
lm(Ylag~Zri,na.omit=TRUE)
lm(Ylag~Zri,na.exclude=TRUE)
?na.omit
lm(Ylag~Zri)
# Exercise 8.10 (self-experiment on the effects of running)#
#
rm(list=ls())       # clear objects in memory#
library(ri)         # load the RI package#
set.seed(1234567)   # random number seed, so that results are reproducible#
library(foreign)    # package allows R to read Stata datasets#
#
# Data are from Hough, Leslie. 2010. “Experimenting with an N of 1.” Yale University Working Paper.#
#
hough <- read.dta("/Users/donaldgreen/Dropbox/Field Experimentation Book/Datasets for Website/Chapter 8_Leslie Hough self-experiment data.dta")#
#
# Part (b)#
#
Y <- hough$tetris#
Z <- hough$run#
#
N <- length(Z)#
#
Zlag <- c(NA,Z[2:N-1]) # exclude day 1 from analysis#
Ylag <- c(NA,Y[2:N-1])#
#
randfun <- function() rbinom(N,1,.5)    # simple random assignment based on coin flips#
numiter <- 10000#
perms <- genperms.custom(numiter=numiter,randfun=randfun)  # random assignment follows the custom function "randfun"#
#
## note on missing data: the default for LM is NA.omit=TRUE#
## This default eliminates the first lagged observation and the two days with missing outcomes#
#
test1 <- lm(Y~Z)$coefficients["Z"]             # regress Y on current Z#
test2 <- summary(lm(Y~Z+Zlag))$fstatistic[1]   # regress Y on current and lagged Z#
test3 <- lm(Ylag~Z)$coefficients["Z"]          # placebo test: regress lagged Y on Z#
test4 <- lm(hough$energy~Z)$coefficients["Z"]  # consider current Z's effects on energy#
test5 <- lm(hough$gre~Z)$coefficients["Z"]     # consider current Z's effects on GRE#
#
# initialize the five vectors of results (all 5 at the same time!)#
testdist1 <- testdist2 <- testdist3 <- testdist4 <- testdist5 <- rep(NA,numiter)#
#
for (i in 1:numiter) {#
	#
	Zri <- perms[,i]#
	Zlagri <- c(NA,Zri[2:N-1]) # exclude day 1 from analysis#
#
testdist1[i] <- lm(Y~Zri)$coefficients["Zri"]#
testdist2[i] <- summary(lm(Y~Zri+Zlagri))$fstatistic[1]#
testdist3[i] <- lm(Ylag~Zri)$coefficients["Zri"]#
testdist4[i] <- lm(hough$energy~Zri)$coefficients["Zri"]#
testdist5[i] <- lm(hough$gre~Zri)$coefficients["Zri"]#
	#
	}#
	#
mean(testdist1 >= test1)      # one-tailed p-value: does running increase Tetris scores#
mean(testdist2 >= test2)      # one-tailed p-value: does running increase Tetris scores#
mean(abs(testdist3) >= abs(test3))  # two-tailed p-value: placebo test#
mean(testdist4 >= test4)      # one-tailed p-value: does running improve energy#
mean(testdist5 >= test5)      # one-tailed p-value: does running improve GRE
